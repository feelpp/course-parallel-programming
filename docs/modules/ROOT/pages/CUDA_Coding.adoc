= CUDA Coding Practice

* Compiling a program for CUDA 
*** For example, to compile MyProg.cu you would use a command like
*** nvcc -o MyProg MyProg.cu


.Array Addition Examples in CPU/GPU
[.examp]
****
* Array Addition Examples in CPU/GPU

.Code Array Addition CPU
[source,cpp]
----
include::ROOT:example$src/Cuda/Array_Addition/01_array_addition_cpu.cpp[indent=0]
----

.Code Array Addition GPU
[source,cu]
----
include::ROOT:example$src/Cuda/Array_Addition/02_array_addition_gpu.cu[indent=0]
----
****


.*Profiling Performance*
****
ADD SOME RESULTS
****






.Array Reduce Examples in CPU
[.examp]
****
* Array Reduce Examples in CPU/GPU

.Code Array Reduce CPU
[source,cpp]
----
include::ROOT:example$src/Cuda/Array_Reduce/01_array_reduce_cpu.cu[indent=0]
----

.Code Array Reduce GPU
[source,cu]
----
include::ROOT:example$src/Cuda/Array_Reduce/02_array_reduce_gpu.cu[indent=0]
----

.Code Array Reduce Atomic GPU
[source,cu]
----
include::ROOT:example$src/Cuda/Array_Reduce/03_array_reduce_gpu_atomic.cu[indent=0]
----

Atomic operations are operations which are performed without interference from any other threads. Atomic operations are often used to prevent race conditions which are common problems in mulithreaded applications. For example, suppose you have two threads named A and B. Now suppose each thread wants to increase the value of memory location 0x1234 by one. Suppose the value at memory location 0x1234 is 5. If A and B both want to increase the value at location 0x1234 at the same time, each thread will first have to read the value. Depending on when the reads occur, it is possible that both A and B will read a value of 5. After adding a value of 1, both A and B will want to write 6 into the memory location, which is not correct! The value, 5, should have been increased twice (once by each thread), but instead, the value was only increased once! This is called a race condition, and can happen in any multi-threaded program if the programmer is not careful.

.Code Array Reduce Shuffle GPU
[source,cu]
----
include::ROOT:example$src/Cuda/Array_Reduce/04_array_reduce_gpu_shuffle.cu[indent=0]
----

.Code Array Reduce Parallelism GPU
[source,cu]
----
include::ROOT:example$src/Cuda/Array_Reduce/05_array_reduce_gpu_parallelism.cu[indent=0]
----

.Code Array Reduce Static GPU
[source,cu]
----
include::ROOT:example$src/Cuda/Array_Reduce/06_array_reduce_gpu_static.cu[indent=0]
----

****

.*Profiling Performance*
****
ADD SOME RESULTS
****








.Matrix SummationExamples in GPU
[.examp]
****
* Matrix SummationExamples in GPU

.Code GPU Grid Info
[source,cu]
----
include::ROOT:example$src/Cuda/Matrix_Summation/01_GPU_grid_block_thread_info.cu[indent=0]
----

.Code Matrix Thread Index Info
[source,cu]
----
include::ROOT:example$src/Cuda/Matrix_Summation/02_matrix_thread_index_info.cu[indent=0]
----

.Code Matrix Summation
[source,cu]
----
include::ROOT:example$src/Cuda/Matrix_Summation/03_matrix_summation_GPU_2D2D_2D1D_1D1D.cu[indent=0]
----



****
.*Profiling Performance*
****
ADD SOME RESULTS
****



.Parallel reduction Examples in CPU/GPU
[.examp]
****
* Parallel reduction Examples in CPU/GPU

.Code Parallel Reduction CPU1
[source,cpp]
----
include::ROOT:example$src/Cuda/Parallel_Reduction/reduction_cpu_1.cpp[indent=0]
----

.Code Parallel Reduction CPU2
[source,cpp]
----
include::ROOT:example$src/Cuda/Parallel_Reduction/reduction_cpu_2.cpp[indent=0]
----

.Code Parallel Reduction GPU1
[source,cu]
----
include::ROOT:example$src/Cuda/Parallel_Reduction/reduction_gpu_1.cu[indent=0]
----

.Code Parallel Reduction GPU2
[source,cu]
----
include::ROOT:example$src/Cuda/Parallel_Reduction/reduction_gpu_2.cu[indent=0]
----

.Code Parallel Reduction GPU3
[source,cu]
----
include::ROOT:example$src/Cuda/Parallel_Reduction/reduction_gpu_3.cu[indent=0]
----

.Code Parallel Reduction GPU4
[source,cu]
----
include::ROOT:example$src/Cuda/Parallel_Reduction/reduction_gpu_4.cu[indent=0]
----

.Code Parallel Reduction GPU5
[source,cu]
----
include::ROOT:example$src/Cuda/Parallel_Reduction/reduction_gpu_5.cu[indent=0]
----

.Code Parallel Reduction GPU6
[source,cu]
----
include::ROOT:example$src/Cuda/Parallel_Reduction/reduction_gpu_6.cu[indent=0]
----

.Code Parallel Reduction GPU7
[source,cu]
----
include::ROOT:example$src/Cuda/Parallel_Reduction/reduction_gpu_7.cu[indent=0]
----




****
.*Profiling Performance*
****
ADD SOME RESULTS
****


.Task Parallelism Examples in CPU/GPU
[.examp]
****
* Task Parallelism Examples in CPU/GPU

.Code Task Parallelism Async Vers1 CPP
[source,cpp]
----
include::ROOT:example$src/Cuda/Task_Parallelism/Async1/Solution/async_cpu.cpp[indent=0]
----

.Code Task Parallelism Async Vers1 GPU
[source,cu]
----
include::ROOT:example$src/Cuda/Task_Parallelism/Async1/Solution/async_gpu_1.cu[indent=0]
----

.Code Task Parallelism Async Vers1 GPU
[source,cu]
----
include::ROOT:example$src/Cuda/Task_Parallelism/Async1/Solution/async_gpu_2.cu[indent=0]
----


.Code Task Parallelism Async Vers2 CPP
[source,cpp]
----
include::ROOT:example$src/Cuda/Task_Parallelism/Async2/Solution/async_cpu.cpp[indent=0]
----

.Code Task Parallelism Async Vers2 GPU
[source,cu]
----
include::ROOT:example$src/Cuda/Task_Parallelism/Async2/Solution/async_gpu_1.cu[indent=0]
----

.Code Task Parallelism Async Vers2 GPU
[source,cu]
----
include::ROOT:example$src/Cuda/Task_Parallelism/Async2/Solution/async_gpu_2.cu[indent=0]
----

.Code Task Parallelism Async Vers3 GPU
[source,cu]
----
include::ROOT:example$src/Cuda/Task_Parallelism/Async2/Solution/async_gpu_3.cu[indent=0]
----

.Code Task Parallelism Async Vers4 GPU
[source,cu]
----
include::ROOT:example$src/Cuda/Task_Parallelism/Async2/Solution/async_gpu_4.cu[indent=0]
----



****
.*Profiling Performance*
****
ADD SOME RESULTS
****



.Vector Examples in CPU/GPU
[.examp]
****
* Vector Examples in CPU/GPU

****
.*Profiling Performance*
****
ADD SOME RESULTS
****

...

