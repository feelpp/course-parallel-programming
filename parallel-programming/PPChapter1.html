<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Untitled :: Parallel Programming</title>
    <link rel="canonical" href="https://feelpp.github.io/parallel-programming/parallel-programming/PPChapter1.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../_/css/site.css">
<link rel="icon" href="../_/img/favicon.ico" type="image/x-icon">
<script>!function(l,p){if(l.protocol!==p&&l.host=="docs.antora.org"){l.protocol=p}else if(/\.gitlab\.io$/.test(l.host)){l.replace(p+"//docs.antora.org"+l.pathname.substr(l.pathname.indexOf("/",1))+l.search+l.hash)}}(location,"https:")</script>

<script src="../_/js/vendor/tabs-block-extension.js"></script>
<script src="../_/js/vendor/tabs-block-behavior.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },

  TeX: {
      Macros: {
      bold: ["{\\bf #1}",1],
      calTh: "{\\mathcal{T}_h}",
      card: ["{\\operatorname{card}(#1)}",1],
      card: ["{\\operatorname{card}(#1)}",1],
      Ck: ["{\\mathcal{C}^{#1}}",1],
      deformt: ["{\\mathbf{\\varepsilon(#1)}}",1],
      diam: "{\\operatorname{diam}}",
      dim: ["{\\operatorname{dim}(#1)}",1],
      disp: ["{\\mathbf{#1}}",1],
      domain: "{\\Omega}",
      ds: "",
      essinf: "{\\operatorname{ess}\\, \\operatorname{inf}}",
      F:"{\\mathcal{F}}",
      geo: "{\\mathrm{geo}}",
      Ich: ["{\\mathcal{I}^{#1}_{c,h}#2}",2],
      Id: "{\\mathcal{I}}",
      Ilag: ["{\\mathcal{I}^{\\mathrm{lag}}_{#1}}",1],
      jump: ["{[\\![ #1 ]\\!]}",1],
      n:"{\\mathbf{n}}",
      Ne: "{N_{\\mathrm{e}}}",
      Next: "{\\mathrm{n}}",
      nf: "{n_f}",
      ngeo: "{n_{\\mathrm{geo}}}",
      Nma: "{N_{\\mathrm{ma}}}",
      NN: "{\\mathbb N}",
      Nno: "{N_{\\mathrm{no}}}",
      Nso: "{N_{\\mathrm{so}}}",
      opdim: "{\\operatorname{dim}}",
      p: "{\\mathrm{p}}",
      P:"{\\mathcal{P}}",
      Pch: ["{P^{#1}_{c,h}}",1],
      Pcho: ["{P^{#1}_{c,h,0}}",1],
      Pk: ["{\\mathcal{P}^{#1}}",1],
      poly: ["{\\mathbb{#1}",1],
      poly: ["{\\mathbb{#1}}",1],
      prect: ["{\\left\\(#1\\right\\)}",1],
      q:"{\\mathbf{q}}",
      Qch: ["{Q^{#1}_{c,h}}",1],
      Qk: ["{\\mathcal{Q}^{#1}}",1],
      R: ["{\\mathbb{R}^{#1}}",1],
      RR: "{\\mathbb R}",
      set: ["{\\left\\{#1\\right\\}}",1],
      stresst: ["{\\mathbf{\\sigma(#1)}}",1],
      T:"{\\mathcal{T}}",
      tr: "{\\operatorname{tr}}",
      v:"{\\mathbf{v}}",
      vertiii: ["\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert #1 \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert",1]
  },
  extensions: ["mhchem.js"] 
  }
});
</script>
<!--<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>-->
<!-- <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML'></script>
<!--<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.0/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>-->

<!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>-->
<script>var uiRootPath = '../_'</script>

  </head>
  <body class="article">
<header class="header">
    <nav class="navbar navbar-expand-sm bg-dark navbar-dark navbar-template-project" style="border-top: 4px solid #9E9E9E">
        <div class="navbar-brand">
            <div class="navbar-item feelpp-logo">
                <a href="https://feelpp.github.io/parallel-programming">Parallel Programming</a>
            </div>
            <button class="navbar-burger" data-target="topbar-nav">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>

        <div id="topbar-nav" class="navbar-menu">
            <div class="navbar-end">
                <div class="navbar-item">
                    <a href="https://docs.feelpp.org/">Documentation Reference</a>
                </div>
                <div class="navbar-item has-dropdown is-hoverable download-item">
                    <div class="navbar-item"><a href="https://docs.feelpp.org/user/latest/install/index.html" class="download-btn">Get Feel++</a></div>
                </div>
                <div class="navbar-item">
                    <a class="navbar-brand"  href="https://www.cemosis.fr">
                        <img class="cemosis-logo"  src="../_/img/cemosis-logo.svg" alt="Cemosis logo"/>
                    </a>
                </div>
            </div>
        </div>
    </nav>
</header>
<div class="body">
<a href="#" class="menu-expand-toggle"></a>
<div class="nav-container" data-component="parallel-programming" data-version="">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="index.html">Template Project</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="index.html">Introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <span class="nav-text">{Parallel Programming} Environment</span>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Template Project</span>
    <span class="version"></span>
  </div>
  <ul class="components">
      <li class="component">
        <a class="title" href="../feelpp-antora-ui/index.html">Antora Feel++ UI</a>
      </li>
      <li class="component is-current">
        <a class="title" href="index.html">Template Project</a>
      </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
  <button class="nav-toggle"></button>
    <a href="index.html" class="home-link"></a>
  <nav class="breadcrumbs" aria-label="breadcrumbs">
</nav>

  
    <div class="edit-this-page"><a href="https://github.com/feelpp/parallel-programming/edit/lem/docs/modules/ROOT/pages/PPChapter1.adoc">Edit this Page</a></div>
  
  <div class="page-downloads">
  <span class="label">Download as</span>
  <ul class="download-options">
    <li>
      <a onclick="print(this)" href="#" data-toggle="tooltip" data-placement="left" title="Print to PDF"
         class="pdf-download">
        <img class="pdf-file-icon icon" src="../_/img/pdf.svg"/> .pdf
      </a>
    </li>
  </ul>
</div>
</div>

  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<div class="sect2">
<h3 id="_1_cpu_gpu_gpgpu_architecture"><a class="anchor" href="#_1_cpu_gpu_gpgpu_architecture"></a>1. 1. CPU, GPU, GPGPU Architecture</h3>
<div class="paragraph">
<p>CPU, GPU, and GPGPU architectures are all types of computer processing
architectures, but they differ in their design and operation.</p>
</div>
<div class="paragraph">
<p>CPU: A central processor (CPU) is a processing unit that is designed to
perform various computing tasks including data processing, mathematical
and logical calculations, communication between different components of
a computer system, etc. Modern CPUs usually have multiple cores to
process multiple tasks simultaneously.</p>
</div>
<div class="paragraph">
<p>GPU: A graphics processing unit (GPU) is an architecture designed to
accelerate the processing of images and graphics. GPUs have thousands of
cores that allow them to process millions of pixels simultaneously,
making them an ideal choice for video games, 3D modeling, and other
graphics-intensive applications.</p>
</div>
<div class="paragraph">
<p>GPGPU: A General Processing Architecture (GPGPU) is a type of GPU that
is designed to be used for purposes other than graphics processing.
GPGPUs are used to perform computations of an intensive nature using the
hundreds or thousands of cores available on the graphics card. They are
particularly effective for parallel computing, machine learning, and
other computationally intensive areas.</p>
</div>
<div class="paragraph">
<p>In conclusion, the main difference between the three architectures CPU,
GPU and GPGPU lies in their design and operation. While CPUs are
designed for general computer processing, GPUs are designed for
specialized graphics processing, and GPGPUs are a modified version of
GPUs intended to be used for specialized computer processing other than
graphics processing.</p>
</div>
<div class="sect3">
<h4 id="_1_1_cpu"><a class="anchor" href="#_1_1_cpu"></a>1.1. 1.1 CPU</h4>
<div class="paragraph">
<p>The CPU basically consists of three parts:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The control unit which searches for instructions in
memory, decodes them and coordinates the rest of the processor to
execute them. A basic control unit basically consists of an instruction
register and a "decoder/sequencer" unit</p>
</li>
<li>
<p>The Arithmetic and Logic Unit executes the
arithmetic and logic instructions requested by the control unit.
Instructions can relate to one or more operands. The execution speed is
optimal when the operands are located in the registers rather than in
the memory external to the processor.</p>
</li>
<li>
<p>Registers are memory cells internal to the CPU
They are few in number but very quick to access. They
are used to store variables, the intermediate results of operations
(arithmetic or logical) or processor control information.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><span class="image unresolved"><img src="../assets/images/image1.png" alt="image" width="322" height="220"></span></p>
</div>
<div class="paragraph">
<p>The register structure varies from processor to processor. This is why
each type of CPU has its own instruction set. Their basic functions are nevertheless similar and all processors have roughly the same categories of registers:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <strong>accumulator</strong> is primarily intended to hold the data that needs to
be processed by the ALU.</p>
</li>
<li>
<p>General registers* are used to store temporary data and intermediate</p>
</li>
<li>
<p>Address registers* are used to construct particular data addresses.
These are, for example, the base and index registers which allow, among
other things, to organize the data in memory like indexed tables.</p>
</li>
<li>
<p>The <strong>instruction register</strong> contains the code of the instruction which is processed by the decoder/sequencer.</p>
</li>
<li>
<p>The <strong>ordinal counter</strong> contains the address of the next instruction to be executed. In principle, this register never stops counting. It generates the addresses of the instructions to be executed one after the other. Some instructions sometimes require changing the contents of the ordinal counter to make a sequence break, ie a jump elsewhere in the program.</p>
</li>
<li>
<p>The <strong>status register,</strong> sometimes called <strong>the condition register,</strong>
contains indicators called <em>flags</em> whose values (0 or 1) vary according
to the results of the arithmetic and logical operations. These states
are used by conditional jump instructions.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The <strong>stack pointer</strong> or <em>stack pointer</em> manages certain data in memory by
organizing them in the form of stacks.</p>
</div>
<div class="paragraph">
<p><strong>CPU working principle</strong></p>
</div>
<div class="paragraph">
<p>The content of the program counter is deposited on the addressing bus in
order to search there for a machine code instruction. The control bus
produces a read signal and the memory, which is selected by the address,
sends the instruction code back to the processor via the data bus. Once
the instruction lands in the instruction register, the processor&#8217;s
control unit decodes it and produces the appropriate sequence of
internal and external signals that coordinate its execution. An
instruction comprises a series of elementary tasks. They are clocked by
clock cycles.</p>
</div>
<div class="paragraph">
<p>All the tasks that constitute an instruction are executed one after the
other. The execution of an instruction therefore lasts several cycles.
As it is not always possible to increase the frequency, the only way to
increase the number of instructions processed in a given time is to seek
to execute several of them simultaneously. This is achieved by splitting
processor resources, data and/or processes. This is called the
parallelization.</p>
</div>
<div class="paragraph">
<p><strong>The different architectures of the processor</strong></p>
</div>
<div class="paragraph">
<p>There is a classification of the <strong>different CPU architectures.</strong> Five in
number, they are used by programmers depending on the desired results:</p>
</div>
<div class="ulist">
<ul>
<li>
<p></p>
<div class="paragraph">
<p>CISC: very complex addressing;</p>
</div>
</li>
<li>
<p></p>
<div class="paragraph">
<p>RISC: simpler addressing and instructions performed on a single cycle;</p>
</div>
</li>
<li>
<p></p>
<div class="paragraph">
<p>VLIW: long, but simpler instructions;</p>
</div>
</li>
<li>
<p></p>
<div class="paragraph">
<p>vectorial: contrary to the processing in number, the instructions are
vectorial;</p>
</div>
</li>
<li>
<p></p>
<div class="paragraph">
<p>dataflow: data is active unlike other architectures.</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>To further improve the <strong>performance of this processor,</strong> developers can
add so-called SIMD Supplemental Instruction Sets.</p>
</div>
<div class="paragraph">
<p><strong>1. 2 GPU (Graphics Processing Unit is a graphics (co-)processor)</strong></p>
</div>
<div class="paragraph">
<p>Graphics Processing Unit is a graphics (co-)processor capable of very
efficiently performing calculations on images (2D, 3D, videos, etc.).
The raw computing power offered is higher due to the large number of
processors present on these cards. This is why it is not uncommon to
obtain large acceleration factors between CPU and GPU for the same
application.</p>
</div>
<div class="paragraph">
<p>Explicit code targeting GPUs: CUDA, HIP, SYCL, Kokkos, RAJA,&#8230;&#8203;</p>
</div>
<div class="paragraph">
<p><span class="image unresolved"><img src="../assets/images/image2.png" alt="image" width="488" height="342"></span></p>
</div>
<div class="paragraph">
<p><em>Fig: illustrates the main hardware architecture differences between
CPUs and GPUs. The transistor counts associated with various functions
are represented abstractly by the relative sizes of the various shaded
areas. In the figure, the green corresponds to the calculation; gold is
instruction processing; purple is the L1 cache; blue is top level cache
and orange is memory (DRAM, which really should be thousands of times
larger than caches).</em></p>
</div>
<div class="paragraph">
<p>GPUs were originally designed to render graphics. They work great for
shading, texturing, and rendering the thousands of independent polygons
that make up a 3D object. CPUs, on the other hand, are meant to control
the logical flow of any general-purpose program, where a lot of digit
manipulation may (or may not) be involved. Due to these very different
roles, GPUs are characterized by having many more processing units and
higher overall memory bandwidth, while CPUs offer more sophisticated
instruction processing and faster clock speed.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 23%;">
<col style="width: 44%;">
<col style="width: 33%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"></th>
<th class="tableblock halign-left valign-top"><strong>CPU: Latency-oriented design</strong></th>
<th class="tableblock halign-left valign-top"><strong>GPU: Throughput Oriented Design</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Clock</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">High clock frequency</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Moderate clock frequency</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Caches</strong></p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Large sizes</p>
</div>
<div class="paragraph">
<p>Converts high latency accesses in memory to low latency accesses in
cache</p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Small caches</p>
</div>
<div class="paragraph">
<p>To maximize memory throughput</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Control</strong></p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Sophisticated control system</p>
</div>
<div class="paragraph">
<p>Branch prediction to reduce latency due to branching<br>
Data loading to reduce latency due to data access</p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Single controlled</p>
</div>
<div class="paragraph">
<p>No branch prediction</p>
</div>
<div class="paragraph">
<p>No data loading</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Powerful Arithmetic Logic Unit (ALU)</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Reduced operation latency</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Numerous, high latency but heavily pipelined for high throughput</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Other aspects</strong></p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Lots of space devoted to caching and control logic. Multi-level caches
used to avoid latency</p>
</div>
<div class="paragraph">
<p>Limited number of registers due to fewer active threads</p>
</div>
<div class="paragraph">
<p>Control logic to reorganize execution, provide ILP, and minimize
pipeline hangs</p>
</div></div></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Requires a very large number of threads for latency to be tolerable</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Beneficial aspects for applications</strong></p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>CPUs for sequential games where latency is critical.</p>
</div>
<div class="paragraph">
<p>CPUs can be 10+X faster than GPUs for sequential code.</p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>GPUs for parallel parts where throughput is critical.</p>
</div>
<div class="paragraph">
<p>GPUs can be 10+X faster than GPUs for parallel code.</p>
</div></div></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p><strong>1.3 GPGPU ( General-Purpose Graphics Processing Unit)</strong></p>
</div>
<div class="paragraph">
<p><span class="image unresolved"><img src="../assets/images/image4.png" alt="image" width="642" height="331"></span></p>
</div>
<div class="paragraph">
<p>A <strong>General-Purpose Graphics Processing Unit</strong> (GPGPU) is a graphics
processing unit (GPU) that is programmed for purposes beyond graphics
processing, such as performing computations typically conducted by a
Central Processing Unit (CPU).</p>
</div>
<div class="paragraph">
<p><em>GPGPU</em> is short for general-purpose computing on graphics processing
units. Graphics processors or GPUs today are capable of much more than
calculating pixels in video games. For this, Nvidia has been developing
for four years a hardware interface and a programming language derived
from C, CUDA ( <strong>C</strong> ompute <strong>Unified Device Architecture</strong> ). This
technology, known as <strong>GPGPU</strong> ( <strong>General</strong> - <strong>P</strong> urpose computation on <strong>G</strong>
raphic <strong>P</strong> rocessing <strong>Units</strong> ) exploits the computing power of GPUs for
the processing of massively parallel tasks. Unlike the CPU, a GPU is not
suited for fast processing of tasks that run sequentially. On the other
hand, it is very suitable for processing parallelizable algorithms.</p>
</div>
<div class="paragraph">
<p>•Array of independent "cores" called calculation units</p>
</div>
<div class="ulist">
<ul>
<li>
<p>High bandwidth, banked L2 caches and main memory</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>− Banks allow several parallel accesses</p>
</div>
<div class="paragraph">
<p>− 100s of GB/s</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Memory and caches are generally inconsistent</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Compute units are based on SIMD hardware</p>
</div>
<div class="paragraph">
<p>− Both AMD and NVIDIA have 16-element wide SIMDs</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Large registry files are used for fast context switching</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>− No save/restore state</p>
</div>
<div class="paragraph">
<p>− Data is persistent throughout the execution of the thread</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Both providers have a combination of automatic L1 cache and
user-managed scratchpad</p>
</li>
<li>
<p>Scratchpad is heavily loaded and has very high bandwidth
(~terabytes/second)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Work items are automatically grouped into hardware threads called
"wavefronts" (AMD) or "warps" (NVIDIA)</p>
</div>
<div class="paragraph">
<p>− Single instruction stream executed on SIMD hardware</p>
</div>
<div class="paragraph">
<p>− 64 work items in a wavefront, 32 in a string</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The instruction is issued multiple times on the 16-channel SIMD unit</p>
</li>
<li>
<p>Control flow is managed by masking the SIMD channel</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>NVIDIA coined "Single Instruction Multiple Threads" (SIMT) to refer to
multiple (software) threads sharing a stream of instructions</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Work items run in sequence on SIMD hardware</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>− Multiple software threads are executed on a single hardware thread</p>
</div>
<div class="paragraph">
<p>− Divergence between managed threads using predication</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Accuracy is transparent to the OpenCL model</p>
</li>
<li>
<p>Performance is highly dependent on understanding work items to SIMD
mapping</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>1.4 Architecture of a GPU versus CPU</strong></p>
</div>
<div class="paragraph">
<p>Such an architecture is said to be "throughput-oriented". The latest
from the Santa-Clara firm, codenamed “Fermi” has 512 cores.</p>
</div>
<div class="paragraph">
<p><span class="image unresolved"><img src="../assets/images/image5.png" alt="image" width="530" height="241"></span></p>
</div>
<div class="paragraph">
<p><em>CPU architecture vs. GPUs</em></p>
</div>
<div class="paragraph">
<p>Traditional microprocessors (CPUs) are essentially "low latency
oriented". The goal is to minimize the execution time of a single
sequence of a program by reducing latency as much as possible. This
design takes the traditional assumption that parallelism in the
operations that the processor must perform is very rare.</p>
</div>
<div class="paragraph">
<p>Throughput-oriented processors assume that their workload requires
significant parallelism. The idea is not to execute the operations as
quickly as possible sequentially, but to execute billions of operations
simultaneously in a given time, the execution time of one of these
operations is ultimately almost irrelevant. In a video game, for
example, performance is measured in FPS (Frames Per Seconds). To do
this, an image, with all the pixels, must be displayed every 30
milliseconds (approximately). It doesn&#8217;t matter how long a single pixel
is displayed.</p>
</div>
<div class="paragraph">
<p>This type of processor has small independent calculation units which
execute the instructions in the order in which they appear in the
program, there is ultimately little dynamic control over the execution.
Thea term <strong>SIMD</strong> is used for these processors (<strong>S</strong>ingle <strong>I</strong>nstruction <strong>M</strong>ultiple <strong>Da</strong>ta).</p>
</div>
<div class="paragraph">
<p>Each PU (Processing Unit) does not necessarily correspond to a
processor, they are calculation units. In this mode, the same
instruction is applied simultaneously to several data.</p>
</div>
<div class="paragraph">
<p>Less control logic means more space on the chip dedicated to the
calculation. However, this also comes at a cost. A SIMD execution gets a
performance peak when parallel tasks follow the same branch of
execution, which deteriorates when the tasks branch off. Indeed, the
calculation units assigned to a branch will have to wait for the
execution of the calculation units of the previous branch. This results
in hardware underutilization and increased execution time. The
efficiency of the SIMD architecture depends on the uniformity of the
workload.</p>
</div>
<div class="paragraph">
<p>However, due to the large number of computational units, it may not be
very important to have some threads blocked if others can continue their
execution. Long-latency operations performed on one thread are "hidden"
by others ready to execute another set of instructions.</p>
</div>
<div class="paragraph">
<p>For a quad or octo-core CPU, the creation of threads and their
scheduling has a cost. For a GPU, the relative latency "covers" these 2
steps, making them negligible. However, memory transfers have greater
implications for a GPU than a CPU because of the need to move data
between CPU memory and GPU memory.</p>
</div>
<div class="paragraph">
<p>(See:
<a href="https://blog.octo.com/la-technologie-gpgpu-1ere-partie-le-cote-obscur-de-la-geforce/" class="bare">blog.octo.com/la-technologie-gpgpu-1ere-partie-le-cote-obscur-de-la-geforce/</a>
)</p>
</div>
<div class="paragraph">
<p><strong>SIMD (Single Instruction Multiple Data)</strong></p>
</div>
<div class="paragraph">
<p>SIMD is a computer technique that allows several data elements to be
exploited at the same time.</p>
</div>
<div class="paragraph">
<p><strong>What is SIMD used for?</strong></p>
</div>
<div class="paragraph">
<p>SIMD can be used in a wide range of applications, such as 3D graphics,
signal processing, data mining, and many other processing-intensive
tasks. In the realm of 3D graphics, SIMD can be used to process large
amounts of data in parallel, making graphics rendering faster and
smoother. In signal processing, SIMD can be used to process multiple
signals at the same time, thereby increasing the efficiency of signal
processing. In data mining, SIMD can be used to process large volumes of
data in parallel, which makes data mining faster and more efficient.</p>
</div>
<div class="paragraph">
<p>SIMD is also commonly used in encryption and data compression
algorithms. These algorithms often require the processing of large
amounts of data, and SIMD can be used to speed up the process. SIMD can
also be used to process large amounts of data in parallel in machine
learning algorithms such as artificial neural networks.</p>
</div>
<div class="paragraph">
<p><strong>Benefits of using SIMD</strong></p>
</div>
<div class="paragraph">
<p>SIMD has several advantages over other forms of parallelization. First,
SIMD is more efficient than traditional software parallelization
techniques, such as threading. This is because SIMD takes advantage of
the capabilities of modern processors and is optimized for parallelism.
This means that SIMD can process multiple pieces of data in parallel at
the same time, which greatly improves program performance.</p>
</div>
<div class="paragraph">
<p>In addition, SIMD allows more efficient use of memory. Since the same
instruction is applied to multiple pieces of data in parallel, the
amount of memory required to store data is reduced. This can help
improve performance by reducing the amount of memory required to store
data items.</p>
</div>
<div class="paragraph">
<p>Finally, SIMD is more flexible than other forms of parallelization. This
is because SIMD allows the same instruction to be applied to multiple
data items in parallel, allowing the programmer to customize the code
according to application requirements.</p>
</div>
<div class="paragraph">
<p><strong>1.5 AMD ROCm Platform, CUDA</strong></p>
</div>
<div class="paragraph">
<p><strong>1.5.1 AMD ROC platform</strong></p>
</div>
<div class="paragraph">
<p>ROCm™ is a collection of drivers , development tools, and APIs that
enable GPU programming from low-level kernel to end-user applications
<strong>.</strong> ROCm is powered by AMD&#8217;s Heterogeneous Computing Interface for
Portability , an OSS C++ GPU programming environment and its
corresponding runtime environment <strong>.</strong> HIP enables ROCm developers to
build portable applications across different platforms by deploying code
on a range of platforms , from dedicated gaming GPUs to exascale HPC
clusters <strong>.</strong></p>
</div>
<div class="paragraph">
<p>ROCm supports programming models such as OpenMP and OpenCL , and
includes all necessary compilers , debuggers and OSS libraries <strong>.</strong> ROCm
is fully integrated with ML frameworks such as PyTorch and TensorFlow
<strong>.</strong> ROCm can be deployed in several ways , including through the use of
containers such as Docker , Spack, and your own build from source <strong>.</strong></p>
</div>
<div class="paragraph">
<p>ROCm is designed to help develop , test, and deploy GPU-accelerated HPC
, AI , scientific computing , CAD, and other applications in a free ,
open-source , integrated, and secure software ecosystem <strong>.</strong></p>
</div>
<div class="paragraph">
<p><strong>CUDA Platform</strong></p>
</div>
<div class="paragraph">
<p>CUDA® is a parallel computing platform and programming model developed
by NVIDIA for general computing on graphics processing units (GPUs).
With CUDA, developers can dramatically speed up computing applications
by harnessing the power of GPUs.</p>
</div>
<div class="paragraph">
<p>The CUDA architecture is based on a three-level hierarchy of cores,
threads, and blocks. Cores are the basic unit of computation while
threads are the individual pieces of work that the cores work on. Blocks
are collections of threads that are grouped together and can be run
together. This architecture enables efficient use of GPU resources and
makes it possible to run multiple applications at once.</p>
</div>
<div class="paragraph">
<p>The NVIDIA CUDA-X platform, which is built on CUDA®, brings together a
collection of libraries, tools, and technologies that deliver
significantly higher performance than competing solutions in multiple
application areas ranging from artificial intelligence to high
performance computing.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"><strong>GPUs</strong></th>
<th class="tableblock halign-left valign-top"></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>CUDA ( Compute Unified Device Architecture)</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>HIP
("Heterogeneous-Compute Interface for Portability")</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Has been the de facto standard for native GPU code for years</p>
</div>
<div class="paragraph">
<p>Huge set of optimized libraries available</p>
</div>
<div class="paragraph">
<p>Custom syntax (extension of C++) supported only by CUDA compilers</p>
</div>
<div class="paragraph">
<p>Support for NVIDIA devices only</p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>AMD&#8217;s effort to offer a common programming interface that works on both
CUDA and ROCm devices</p>
</div>
<div class="paragraph">
<p>Standard C++ syntax, uses the nvcc/hcc compiler in the background</p>
</div>
<div class="paragraph">
<p>Almost an individual CUDA clone from the user&#8217;s perspective</p>
</div>
<div class="paragraph">
<p>The ecosystem is new and growing rapidly</p>
</div></div></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p><strong>1.5.3 What is the difference between CUDA and ROCm for GPGPU
applications?</strong></p>
</div>
<div class="paragraph">
<p>NVIDIA&#8217;s CUDA and AMD&#8217;s ROCm provide frameworks to take advantage of the
respective GPU platforms.</p>
</div>
<div class="paragraph">
<p>Graphics processing units (GPUs) are traditionally designed to handle
graphics computing tasks, such as image and video processing and
rendering, 2D and 3D graphics, vectorization, etc. General purpose
computing on GPUs became more practical and popular after 2001, with the
advent of programmable shaders and floating point support on graphics
processors.</p>
</div>
<div class="paragraph">
<p>Notably, it involved problems with matrices and vectors, including two-,
three-, or four-dimensional vectors. These were easily translated to
GPU, which acts with native speed and support on these types. A
milestone for general purpose GPUs (GPGPUs) was the year 2003, when a
pair of research groups independently discovered GPU-based approaches
for solving general linear algebra problems on working GPUs faster than
on CPUs.</p>
</div>
<div class="paragraph">
<p><strong>1.6 GPGPU Evolution</strong></p>
</div>
<div class="paragraph">
<p>Early efforts to use GPUs as general-purpose processors required
reframing computational problems in terms of graphics primitives, which
were supported by two major APIs for graphics processors: OpenGL and
DirectX.</p>
</div>
<div class="paragraph">
<p>These were soon followed by NVIDIA&#8217;s CUDA, which allowed programmers to
abandon underlying graphics concepts for more common high-performance
computing concepts, such as OpenCL and other high-end frameworks. This
meant that modern GPGPU pipelines could take advantage of the speed of a
GPU without requiring a complete and explicit conversion of the data to
a graphical form.</p>
</div>
<div class="paragraph">
<p>NVIDIA describes CUDA as a parallel computing platform and application
programming interface (API) that allows software to use specific GPUs
for general-purpose processing. CUDA is a software layer that provides
direct access to the GPU&#8217;s virtual instruction set and parallel
computing elements for running compute cores.</p>
</div>
<div class="paragraph">
<p>Not to be outdone, AMD launched its own general-purpose computing
platform in 2016, dubbed the Radeon Open Compute Ecosystem (ROCm). ROCm
is primarily intended for discrete professional GPUs, such as AMD&#8217;s
Radeon Pro line. However, official support is more extensive and extends
to consumer products, including gaming GPUs.</p>
</div>
<div class="paragraph">
<p>Unlike CUDA, the ROCm software stack can take advantage of multiple
areas, such as general-purpose GPGPU, high-performance computing (HPC),
and heterogeneous computing. It also offers several programming models,
such as HIP (GPU kernel-based programming), OpenMP/Message Passing
Interface (MPI), and OpenCL. These also support microarchitectures,
including RDNA and CDNA, for a myriad of applications ranging from AI
and edge computing to IoT/IIoT.</p>
</div>
<div class="paragraph">
<p><strong>NVIDIA&#8217;s CUDA</strong></p>
</div>
<div class="paragraph">
<p>Most of NVIDIA&#8217;s Tesla and RTX series cards come with a series of CUDA
cores designed to perform multiple calculations at the same time. These
cores are similar to CPU cores, but they are integrated into the GPU and
can process data in parallel. There can be thousands of these cores
embedded in the GPU, making for incredibly efficient parallel systems
capable of offloading CPU-centric tasks directly to the GPU.</p>
</div>
<div class="paragraph">
<p>Parallel computing is described as the process of breaking down larger
problems into smaller, independent parts that can be executed
simultaneously by multiple processors communicating through shared
memory. These are then combined at the end as part of an overall
algorithm. The primary purpose of parallel computing is to increase
available computing power to speed up application processing and problem
solving.</p>
</div>
<div class="paragraph">
<p>To this end, the CUDA architecture is designed to work with programming
languages such as C, C++ and Fortran, allowing parallel programmers to
more easily utilize GPU resources. This contrasts with previous APIs
such as Direct3D and OpenGL, which required advanced graphics
programming skills. CUDA-powered GPUs also support programming
frameworks such as OpenMP, OpenACC, OpenCL, and HIP by compiling this
code on CUDA.</p>
</div>
<div class="paragraph">
<p>As with most APIs, software development kits (SDKs), and software
stacks, NVIDIA provides libraries, compiler directives, and extensions
for the popular programming languages mentioned earlier, making
programming easier and more effective. These include cuSPARCE, NVRTC
runtime compilation, GameWorks Physx, MIG multi-instance GPU support,
cuBLAS and many more.</p>
</div>
<div class="paragraph">
<p>A good portion of these software stacks are designed to handle AI-based
applications, including machine learning and deep learning, computer
vision, conversational AI, and recommender systems.</p>
</div>
<div class="paragraph">
<p>Computer vision applications use deep learning to acquire knowledge from
digital images and videos. Conversational AI applications help computers
understand and communicate through natural language. Recommender systems
use a user&#8217;s images, language, and interests to deliver meaningful and
relevant search results and services.</p>
</div>
<div class="paragraph">
<p>GPU-accelerated deep learning frameworks provide a level of flexibility
to design and train custom neural networks and provide interfaces for
commonly used programming languages. All major deep learning frameworks,
such as TensorFlow, PyTorch, and others, are already GPU-accelerated, so
data scientists and researchers can upgrade without GPU programming.</p>
</div>
<div class="paragraph">
<p>Current use of the CUDA architecture that goes beyond AI includes
bioinformatics, distributed computing, simulations, molecular dynamics,
medical analytics (CTI, MRI and other scanning imaging applications ),
encryption, etc.</p>
</div>
<div class="paragraph">
<p><strong>AMD&#8217;s ROCm Software Stack</strong></p>
</div>
<div class="paragraph">
<p>AMD&#8217;s ROCm software stack is similar to the CUDA platform, except it&#8217;s
open source and uses the company&#8217;s GPUs to speed up computational tasks.
The latest Radeon Pro W6000 and RX6000 series cards are equipped with
compute cores, ray accelerators (ray tracing) and stream processors that
take advantage of RDNA architecture for parallel processing, including
GPGPU, HPC, HIP (CUDA-like programming model), MPI and OpenCL.</p>
</div>
<div class="paragraph">
<p>Since the ROCm ecosystem is composed of open technologies, including
frameworks (TensorFlow/PyTorch), libraries (MIOpen/Blas/RCCL),
programming models (HIP), interconnects (OCD), and support upstream
Linux kernel load, the platform is regularly optimized. for performance
and efficiency across a wide range of programming languages.</p>
</div>
<div class="paragraph">
<p>AMD&#8217;s ROCm is designed to scale, meaning it supports multi-GPU computing
in and out of server-node communication via Remote Direct Memory Access
(RDMA), which offers the ability to directly access host memory without
CPU intervention. Thus, the more RAM the system has, the greater the
processing loads that can be handled by ROCm.</p>
</div>
<div class="paragraph">
<p>ROCm also simplifies the stack when the driver directly integrates
support for RDMA peer synchronization, making application development
easier. Additionally, it includes ROCr System Runtime, which is language
independent and leverages the HAS (Heterogeneous System Architecture)
Runtime API, providing a foundation for running programming languages
such as HIP and OpenMP.</p>
</div>
<div class="paragraph">
<p>As with CUDA, ROCm is an ideal solution for AI applications, as some
deep learning frameworks already support a ROCm backend (e.g.
TensorFlow, PyTorch, MXNet, ONNX, CuPy, etc.). According to AMD, any
CPU/GPU vendor can take advantage of ROCm, as it is not a proprietary
technology. This means that code written in CUDA or another platform can
be ported to vendor-neutral HIP format, and from there users can compile
code for the ROCm platform.</p>
</div>
<div class="paragraph">
<p>The company offers a series of libraries, add-ons and extensions to
deepen the functionality of ROCm, including a solution (HCC) for the C++
programming language that allows users to integrate CPU and GPU in a
single file.</p>
</div>
<div class="paragraph">
<p>The feature set for ROCm is extensive and incorporates multi-GPU support
for coarse-grained virtual memory, the ability to handle concurrency and
preemption, HSA and atomic signals, DMA and queues in user mode. It also
offers standardized loader and code object formats, dynamic and offline
compilation support, P2P multi-GPU operation with RDMA support, event
tracking and collection API, as well as APIs and system management
tools. On top of that, there is a growing third-party ecosystem that
bundles custom ROCm distributions for a given application across a host
of Linux flavors.</p>
</div>
<div class="paragraph">
<p>To further enhance the capability of exascale systems, AMD also
announced the availability of its open source platform, AMD ROCm, which
enables researchers to harness the power of AMD Instinct accelerators
and drive scientific discovery. Built on the foundation of portability,
the ROCm platform is capable of supporting environments from multiple
vendors and accelerator architectures.</p>
</div>
<div class="paragraph">
<p>And with ROCm5.0, AMD extends its open platform powering the best HPC
and AI applications with AMD Instinct MI200 series accelerators,
increasing ROCm accessibility for developers and delivering
industry-leading performance on workloads keys. And with AMD Infinity
Hub, researchers, data scientists, and end users can easily find,
download, and install containerized HPC applications and ML frameworks
optimized and supported on AMD Instinct and ROCm.</p>
</div>
<div class="paragraph">
<p>The hub currently offers a range of containers supporting Radeon
Instinct™ MI50, AMD Instinct™ MI100, or AMD Instinct MI200 accelerators,
including several applications such as Chroma, CP2k, LAMMPS, NAMD,
OpenMM, etc., as well as frameworks Popular TensorFlow and PyTorch MLs.
New containers are continually being added to the hub.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_amd_fusion_system_architecture"><a class="anchor" href="#_amd_fusion_system_architecture"></a>2. AMD Fusion System Architecture</h3>

</div>
<div class="sect2">
<h3 id="_moves_to_unify_cpus_and_gpus"><a class="anchor" href="#_moves_to_unify_cpus_and_gpus"></a>3. Moves to Unify CPUs and GPUs</h3>
<div class="paragraph">
<p><span class="image unresolved"><img src="../assets/images/image6.png" alt="image" width="511" height="287"></span></p>
</div>
<div class="paragraph">
<p><strong>1.7 TPU (Tensor Processing Unit) form Google</strong></p>
</div>
<div class="paragraph">
<p>A Tensor Processing Unit (TPU) is a specialized hardware processor
developed by Google to accelerate machine learning. Unlike traditional
CPUs or GPUs, TPUs are specifically designed to handle tensor
operations, which account for most of the computations in deep learning
models. This makes them incredibly efficient at those tasks and provides
an enormous speedup compared to CPUs and GPUs. In this article, we’ll
explore what a TPU is, how it works, and why they are so beneficial for
machine learning applications.</p>
</div>
<div class="paragraph">
<p><strong>What Are Tensor Processing Units (TPU)?</strong></p>
</div>
<div class="paragraph">
<p>Tensor Processing Unit (TPU) is an application-specific integrated
circuit (ASIC) designed specifically for machine learning. In addition,
TPUs offer improved energy efficiency, allowing businesses to reduce
their electricity bills while still achieving the same results as
processors with greater energy consumption<strong>.</strong> This makes them an
attractive option for companies looking to use AI in their products or
services<strong>.</strong> With the help of TPUs, businesses can develop and deploy
faster, more efficient models that are better suited to their needs<strong>.</strong>
TPUs offer a range of advantages over CPUs and GPUs<strong>.</strong> For instance,
they provide up to 30x faster performsance than traditional processors
and up to 15x better energy efficiency<strong>.</strong> This makes them ideal for
companies looking to develop complex models in a fraction of the
time<strong>.</strong> Finally, TPUs are more affordable than other specialized
hardware solutions, making them an attractive option for businesses of
all sizes<strong>.</strong></p>
</div>
<div class="paragraph">
<p>Tensor Processing Units are Google&#8217;s ASIC for machine learning. TPUs are
specifically used for deep learning to solve complex matrix and vector
operations. TPUs are streamlined to solve matrix and vector operations
at ultra-high speeds but must be paired with a CPU to give and execute
instructions.</p>
</div>
<div class="paragraph">
<p><span class="image unresolved"><img src="../assets/images/image22.png" alt="image" width="544" height="419"></span></p>
</div>
<div class="paragraph">
<p><strong>Applications for TPUs</strong></p>
</div>
<div class="paragraph">
<p>TPUs can be used in various deep learning applications such as fraud
detection, computer vision, natural language processing, self-driving
cars, vocal AI, agriculture, virtual assistants, stock trading,
e-commerce, and various social predictions.s</p>
</div>
<div class="paragraph">
<p><strong><em>When to Use TPUss</em></strong></p>
</div>
<div class="paragraph">
<p>Since TPUs are high specialized hardware for deep learning, it loses a
lot of other functions you would typically expect from a general-purpose
processor like a CPU. With this in mind, there are specific scenarios
where using TPUs will yield the best result when training AI. The best
time to use a TPU is for operations where models rely heavily on matrix
computations, like recommendation systems for search engines. TPUs also
yield great results for models where the AI analyzes massive amounts of
data points that will take multiple weeks or months to complete. AI
engineers use TPUs for instances without custom TensorFlow models and
have to start from scratch.</p>
</div>
<div class="paragraph">
<p><strong><em>When Not to Use TPUs</em></strong></p>
</div>
<div class="paragraph">
<p>As stated earlier, the optimization of TPUs causes these types of
processors to only work on specific workload operations. Therefore,
there are instances where opting to use a traditional CPU and GPU will
yield faster results. These instances include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Rapid prototyping with maximum flexibility</p>
</li>
<li>
<p>Models limited by the available data points</p>
</li>
<li>
<p>Models that are simple and can be trained quickly</p>
</li>
<li>
<p>Models too onerous to change</p>
</li>
<li>
<p>Models reliant on custom TensorFlow operations written in C++</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 14%;">
<col style="width: 86%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"><strong>TPU Versions and Specifications</strong></th>
<th class="tableblock halign-left valign-top"></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">TPUv1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The first publicly announced TPU. Designed as an 8-bit matrix
multiplication engine and is limited to solving only integers.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">TPUv2:</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Since engineers noted that TPUv1 was limited in bandwidth. This
version now has double the memory bandwidth with 16GB of RAM. This
version can now solve floating points making it useful for training and
inferencing.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">TPUv3</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Released in 2018, TPUv3 has twice the processors and is deployed
with four times as many chips as TPUv2. The upgrades allow this version
to have eight times the performance over previous versions.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">TPUv4</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">This is the latest version of TPU announced on May 18, 2021.
Google&#8217;s CEO announced that this version would have more than twice the
performance of TPU v3.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Edge TPU</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">This TPU version is meant for smaller operations optimized to
use less power than other versions of TPU in overall operation. Although
only using two watts of power, Edge TPU can solve up to four
terra-operations per second. Edge TPU is only found on small handheld
devices like Google&#8217;s Pixel 4 smartphone.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 26%;">
<col style="width: 74%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"><strong>Benefits of the TPU Architecture</strong></th>
<th class="tableblock halign-left valign-top"></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">High Performance:</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed to maximize
performance, ensuring that the processor can execute operations at
extremely high speeds.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Low Power Consumption:</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Compared to CPUs and GPUs, the TPU architecture
requires significantly less power consumption, making it ideal for
applications in which energy efficiency is a priority.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cost Savings:</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed to be affordable,
making it an attractive solution for businesses that are looking to
reduce their hardware costs.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Scalability</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is highly scalable and can
accommodate a wide range of workloads, from small applications to
large-scale projects.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Flexibility</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is flexible and can be adapted to
meet the needs of different applications, making it suitable for a range
of use cases.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Efficient Training</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture enables efficient training of
deep learning models, allowing businesses to quickly iterate and improve
their AI solutions.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Security</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is highly secure, making it an ideal
solution for mission-critical applications that require high levels of
security.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Enhanced Reliability</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture has enhanced reliability,
providing businesses with the assurance that their hardware will perform
as expected in any environment.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Easy to Deploy</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed for easy deployment,
allowing businesses to quickly set up and deploy their hardware
solutions.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Open Source Support</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is backed by an open-source
community that provides support and assistance when needed, making it
easier for businesses to get the most out of their hardware investments.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Improved Efficiency</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed to optimize
efficiency, allowing businesses to get the most out of their hardware
resources and reducing the cost of running AI applications.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">End-to-End Solutions:</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture provides a complete
end-to-end solution for all types of AI projects, allowing businesses to
focus on their development and operations instead of worrying about
hardware compatibility.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cross-Platform Support</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed to work across
multiple platforms, making it easier for businesses to deploy their AI
solutions in any environment.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Future Ready</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed with the future in mind,
providing businesses with a solution that will remain up-to-date and
ready to take on next-generation AI applications.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Industry Standard</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is becoming an industry
standard for AI applications, giving businesses the confidence that
their hardware investments are future-proofed.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p><strong>Applications of the TPU</strong></p>
</div>
<div class="paragraph">
<p>Tensor Processing Units (TPUs) are specialized ASIC chips designed to
accelerate the performance of machine learning algorithms. They can be
used in a variety of applications, ranging from cloud computing and edge
computing to machine learning. TPUs provide an efficient way to process
data, making them suitable for a range of tasks such as image
recognition, language processing, and speech recognition. By leveraging
the power of TPUs, organizations can reduce costs and optimize their
operations.</p>
</div>
<div class="paragraph">
<p><strong>Cloud Computing:</strong> TPUs are used in cloud computing to provide better
performance for workloads that require a lot of data processing. This
allows businesses to process large amounts of data quickly and
accurately at a lower cost than ever before. With the help of TPUs,
businesses can make more informed decisions faster and improve their
operational efficiency.</p>
</div>
<div class="paragraph">
<p><strong>Edge Computing:</strong> TPUs are also used in edge computing applications,
which involve processing data at or near the source. This helps to
reduce latency and improve performance for tasks such as streaming audio
or video, autonomous driving, robotic navigation, and predictive
analytics. Edge computing also facilitates faster and more reliable
communication between devices in an IoT network.</p>
</div>
<div class="paragraph">
<p><strong>Machine Learning:</strong> TPUs are used to accelerate machine learning models
and algorithms. They can be used to develop novel architectures that are
optimized for tasks such as natural language processing, image
recognition, and speech recognition. By leveraging the power of TPUs,
organizations can develop more complex models and algorithms faster.
This will enable them to achieve better results with their
machine-learning applications.</p>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer" style="border-top: 2px solid #e9e9e9; background-color: #fafafa; padding-bottom: 2em; padding-top: 2em;">
    <div class="container" style="display: flex; flex-direction: column; align-items: center; gap: 0.5em;">
        <div>
            <a href="https://www.cemosis.fr">
                <img src="../_/img/cemosis-logo.svg" alt="Cemosis logo" height="50">
            </a>
        </div>
        <span style="font-size: 0.8rem; color: #9e9e9e">© 2023 <a href="https://www.cemosis.fr" style="text-decoration: underline;">Cemosis</a>, Université de Strasbourg</span>
    </div>
</footer>
<script id="site-script" src="../_/js/site.js" data-ui-root-path="../_"></script>


<script async src="../_/js/vendor/fontawesome-icon-defs.js"></script>
<script async src="../_/js/vendor/fontawesome.js"></script>
<script async src="../_/js/vendor/highlight.js"></script>


<script type="text/javascript">
function toggleFullScreen() {
   var doc = window.document;
   var docEl = doc.documentElement;

   var requestFullScreen = docEl.requestFullscreen || docEl.mozRequestFullScreen || docEl.webkitRequestFullScreen || docEl.msRequestFullscreen;
   var cancelFullScreen = doc.exitFullscreen || doc.mozCancelFullScreen || doc.webkitExitFullscreen || doc.msExitFullscreen;

   if(!doc.fullscreenElement && !doc.mozFullScreenElement && !doc.webkitFullscreenElement && !doc.msFullscreenElement) {
       requestFullScreen.call(docEl);
   }
   else {
       cancelFullScreen.call(doc);
   }
}
</script>
  </body>
</html>
