<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>TPU (Tensor Processing Unit) form Google :: Parallel Programming</title>
    <link rel="canonical" href="https://feelpp.github.io/parallel-programming/parallel-programming/PPChapter1_TPU.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../_/css/site.css">
<link rel="icon" href="../_/img/favicon.ico" type="image/x-icon">
<script>!function(l,p){if(l.protocol!==p&&l.host=="docs.antora.org"){l.protocol=p}else if(/\.gitlab\.io$/.test(l.host)){l.replace(p+"//docs.antora.org"+l.pathname.substr(l.pathname.indexOf("/",1))+l.search+l.hash)}}(location,"https:")</script>

<script src="../_/js/vendor/tabs-block-extension.js"></script>
<script src="../_/js/vendor/tabs-block-behavior.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },

  TeX: {
      Macros: {
      bold: ["{\\bf #1}",1],
      calTh: "{\\mathcal{T}_h}",
      card: ["{\\operatorname{card}(#1)}",1],
      card: ["{\\operatorname{card}(#1)}",1],
      Ck: ["{\\mathcal{C}^{#1}}",1],
      deformt: ["{\\mathbf{\\varepsilon(#1)}}",1],
      diam: "{\\operatorname{diam}}",
      dim: ["{\\operatorname{dim}(#1)}",1],
      disp: ["{\\mathbf{#1}}",1],
      domain: "{\\Omega}",
      ds: "",
      essinf: "{\\operatorname{ess}\\, \\operatorname{inf}}",
      F:"{\\mathcal{F}}",
      geo: "{\\mathrm{geo}}",
      Ich: ["{\\mathcal{I}^{#1}_{c,h}#2}",2],
      Id: "{\\mathcal{I}}",
      Ilag: ["{\\mathcal{I}^{\\mathrm{lag}}_{#1}}",1],
      jump: ["{[\\![ #1 ]\\!]}",1],
      n:"{\\mathbf{n}}",
      Ne: "{N_{\\mathrm{e}}}",
      Next: "{\\mathrm{n}}",
      nf: "{n_f}",
      ngeo: "{n_{\\mathrm{geo}}}",
      Nma: "{N_{\\mathrm{ma}}}",
      NN: "{\\mathbb N}",
      Nno: "{N_{\\mathrm{no}}}",
      Nso: "{N_{\\mathrm{so}}}",
      opdim: "{\\operatorname{dim}}",
      p: "{\\mathrm{p}}",
      P:"{\\mathcal{P}}",
      Pch: ["{P^{#1}_{c,h}}",1],
      Pcho: ["{P^{#1}_{c,h,0}}",1],
      Pk: ["{\\mathcal{P}^{#1}}",1],
      poly: ["{\\mathbb{#1}",1],
      poly: ["{\\mathbb{#1}}",1],
      prect: ["{\\left\\(#1\\right\\)}",1],
      q:"{\\mathbf{q}}",
      Qch: ["{Q^{#1}_{c,h}}",1],
      Qk: ["{\\mathcal{Q}^{#1}}",1],
      R: ["{\\mathbb{R}^{#1}}",1],
      RR: "{\\mathbb R}",
      set: ["{\\left\\{#1\\right\\}}",1],
      stresst: ["{\\mathbf{\\sigma(#1)}}",1],
      T:"{\\mathcal{T}}",
      tr: "{\\operatorname{tr}}",
      v:"{\\mathbf{v}}",
      vertiii: ["\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert #1 \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert",1]
  },
  extensions: ["mhchem.js"] 
  }
});
</script>
<!--<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>-->
<!-- <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML'></script>
<!--<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.0/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>-->

<!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>-->
<script>var uiRootPath = '../_'</script>

  </head>
  <body class="article">
<header class="header">
    <nav class="navbar navbar-expand-sm bg-dark navbar-dark navbar-template-project" style="border-top: 4px solid #9E9E9E">
        <div class="navbar-brand">
            <div class="navbar-item feelpp-logo">
                <a href="https://feelpp.github.io/parallel-programming">Parallel Programming</a>
            </div>
            <button class="navbar-burger" data-target="topbar-nav">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>

        <div id="topbar-nav" class="navbar-menu">
            <div class="navbar-end">
                <div class="navbar-item">
                    <a href="https://docs.feelpp.org/">Documentation Reference</a>
                </div>
                <div class="navbar-item has-dropdown is-hoverable download-item">
                    <div class="navbar-item"><a href="https://docs.feelpp.org/user/latest/install/index.html" class="download-btn">Get Feel++</a></div>
                </div>
                <div class="navbar-item">
                    <a class="navbar-brand"  href="https://www.cemosis.fr">
                        <img class="cemosis-logo"  src="../_/img/cemosis-logo.svg" alt="Cemosis logo"/>
                    </a>
                </div>
            </div>
        </div>
    </nav>
</header>
<div class="body">
<a href="#" class="menu-expand-toggle"></a>
<div class="nav-container" data-component="parallel-programming" data-version="">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="index.html">Main</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="index.html">Introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter1_CPU.html">CPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter1_GPU.html">GPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter1_GPGPU.html">GPGPU Architecture</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="PPChapter1_TPU.html">TPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter1_SIMD.html">SIMD Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter2_MPI.html">MPI (Message Passing Interface)</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter2_MPI_Boost.html">MPI Boost</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter2_OpenMP.html">OpenMP (Open Multi-Processing)</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter2_OpenMP2.html">OpenMP more information</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter2_Hybrid.html">Hybrid MPI with OpenMP</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter3.html">StarPU</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter4.html">Specx</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Main</span>
    <span class="version"></span>
  </div>
  <ul class="components">
      <li class="component">
        <a class="title" href="../feelpp-antora-ui/index.html">Antora Feel++ UI</a>
      </li>
      <li class="component is-current">
        <a class="title" href="index.html">Main</a>
      </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
  <button class="nav-toggle"></button>
    <a href="index.html" class="home-link"></a>
  <nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="index.html">Main</a></li>
    <li><a href="PPChapter1_TPU.html">TPU Architecture</a></li>
  </ul>
</nav>

  
    <div class="edit-this-page"><a href="https://github.com/feelpp/parallel-programming/edit/lem/docs/modules/ROOT/pages/PPChapter1_TPU.adoc">Edit this Page</a></div>
  
  <div class="page-downloads">
  <span class="label">Download as</span>
  <ul class="download-options">
    <li>
      <a onclick="print(this)" href="#" data-toggle="tooltip" data-placement="left" title="Print to PDF"
         class="pdf-download">
        <img class="pdf-file-icon icon" src="../_/img/pdf.svg"/> .pdf
      </a>
    </li>
  </ul>
</div>
</div>

  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="3">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">TPU (Tensor Processing Unit) form Google</h1>
<div id="preamble">
<div class="sectionbody">
<div class="imageblock">
<div class="content">
<a class="image" href="#fragment03"><img src="_images/GPGPU.jpg" alt="GPGPU" width="322" height="220"></a>
</div>
</div>
<div class="paragraph text-justify">
<p>A Tensor Processing Unit (TPU) is a specialized hardware processor developed by Google to accelerate machine learning. Unlike traditional CPUs or GPUs, TPUs are specifically designed to handle tensor operations, which account for most of the computations in deep learning models. This makes them incredibly efficient at those tasks and provides an enormous speedup compared to CPUs and GPUs. In this article, weâ€™ll explore what a TPU is, how it works, and why they are so beneficial for machine learning applications.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_what_are_tensor_processing_units_tpu"><a class="anchor" href="#_what_are_tensor_processing_units_tpu"></a>1. What Are Tensor Processing Units (TPU)?</h2>
<div class="sectionbody">
<div class="paragraph text-justify">
<p>Tensor Processing Unit (TPU) is an application-specific integrated
circuit (ASIC) designed specifically for machine learning. In addition, TPUs offer improved energy efficiency, allowing businesses to reduce their electricity bills while still achieving the same results as processors with greater energy consumption. This makes them an attractive option for companies looking to use AI in their products or services. With the help of TPUs, businesses can develop and deploy
faster, more efficient models that are better suited to their needs. TPUs offer a range of advantages over CPUs and GPUs. For instance, they provide up to 30x faster performsance than traditional processors and up to 15x better energy efficiency. This makes them ideal for companies looking to develop complex models in a fraction of the time. Finally, TPUs are more affordable than other specialized hardware solutions, making them an attractive option for businesses of all sizes.</p>
</div>
<div class="paragraph text-justify">
<p>Tensor Processing Units are Google&#8217;s ASIC for machine learning. TPUs are specifically used for deep learning to solve complex matrix and vector operations. TPUs are streamlined to solve matrix and vector operations at ultra-high speeds but must be paired with a CPU to give and execute instructions.</p>
</div>
<div class="paragraph">
<p><span class="image"><a class="image" href="#fragment22"><img src="_images/image22.png" alt="image22" width="544" height="419"></a></span></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_applications_for_tpus"><a class="anchor" href="#_applications_for_tpus"></a>2. Applications for TPUs</h2>
<div class="sectionbody">
<div class="paragraph text-justify">
<p>TPUs can be used in various deep learning applications such as fraud detection, computer vision, natural language processing, self-driving cars, vocal AI, agriculture, virtual assistants, stock trading, e-commerce, and various social predictions.s</p>
</div>
<div class="paragraph">
<p><strong><em>When to Use TPUs</em></strong></p>
</div>
<div class="paragraph text-justify">
<p>Since TPUs are high specialized hardware for deep learning, it loses a
lot of other functions you would typically expect from a general-purpose processor like a CPU. With this in mind, there are specific scenarios where using TPUs will yield the best result when training AI. The best time to use a TPU is for operations where models rely heavily on matrix computations, like recommendation systems for search engines. TPUs also yield great results for models where the AI analyzes massive amounts of data points that will take multiple weeks or months to complete. AI engineers use TPUs for instances without custom TensorFlow models and have to start from scratch.</p>
</div>
<div class="paragraph">
<p><strong><em>When Not to Use TPUs</em></strong></p>
</div>
<div class="paragraph text-justify">
<p>As stated earlier, the optimization of TPUs causes these types of processors to only work on specific workload operations. Therefore, there are instances where opting to use a traditional CPU and GPU will yield faster results. These instances include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Rapid prototyping with maximum flexibility</p>
</li>
<li>
<p>Models limited by the available data points</p>
</li>
<li>
<p>Models that are simple and can be trained quickly</p>
</li>
<li>
<p>Models too onerous to change</p>
</li>
<li>
<p>Models reliant on custom TensorFlow operations written in C++</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 14%;">
<col style="width: 86%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>TPU Versions and Specifications</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">[.text-justify]</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">TPUv1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The first publicly announced TPU. Designed as an 8-bit matrix multiplication engine and is limited to solving only integers.
[.text-justify]</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">TPUv2:</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Since engineers noted that TPUv1 was limited in bandwidth. This version now has double the memory bandwidth with 16GB of RAM. This version can now solve floating points making it useful for training and inferencing.
[.text-justify]</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">TPUv3</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Released in 2018, TPUv3 has twice the processors and is deployed with four times as many chips as TPUv2. The upgrades allow this version to have eight times the performance over previous versions.
[.text-justify]</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">TPUv4</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">This is the latest version of TPU announced on May 18, 2021. Google&#8217;s CEO announced that this version would have more than twice the performance of TPU v3.
[.text-justify]</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Edge TPU</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">This TPU version is meant for smaller operations optimized to use less power than other versions of TPU in overall operation. Although only using two watts of power, Edge TPU can solve up to four terra-operations per second. Edge TPU is only found on small handheld
devices like Google&#8217;s Pixel 4 smartphone.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 26%;">
<col style="width: 74%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"><strong>Benefits of the TPU Architecture</strong></th>
<th class="tableblock halign-left valign-top"></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">High Performance:</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed to maximize performance, ensuring that the processor can execute operations at extremely high speeds.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Low Power Consumption:</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Compared to CPUs and GPUs, the TPU architecture requires significantly less power consumption, making it ideal for applications in which energy efficiency is a priority.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cost Savings:</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed to be affordable, making it an attractive solution for businesses that are looking to reduce their hardware costs.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Scalability</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is highly scalable and can
accommodate a wide range of workloads, from small applications to large-scale projects.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Flexibility</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is flexible and can be adapted to meet the needs of different applications, making it suitable for a range of use cases.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Efficient Training</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture enables efficient training of deep learning models, allowing businesses to quickly iterate and improve their AI solutions.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Security</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is highly secure, making it an ideal solution for mission-critical applications that require high levels of security.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Enhanced Reliability</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture has enhanced reliability, providing businesses with the assurance that their hardware will perform as expected in any environment.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Easy to Deploy</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed for easy deployment, allowing businesses to quickly set up and deploy their hardware solutions.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Open Source Support</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is backed by an open-source community that provides support and assistance when needed, making it easier for businesses to get the most out of their hardware investments.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Improved Efficiency</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed to optimize efficiency, allowing businesses to get the most out of their hardware resources and reducing the cost of running AI applications.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">End-to-End Solutions:</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture provides a complete end-to-end solution for all types of AI projects, allowing businesses to focus on their development and operations instead of worrying about hardware compatibility.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cross-Platform Support</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed to work across
multiple platforms, making it easier for businesses to deploy their AI
solutions in any environment.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Future Ready</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed with the future in mind, providing businesses with a solution that will remain up-to-date and ready to take on next-generation AI applications.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Industry Standard</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is becoming an industry standard for AI applications, giving businesses the confidence that their hardware investments are future-proofed.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1">
<h2 id="_applications_of_the_tpu"><a class="anchor" href="#_applications_of_the_tpu"></a>3. Applications of the TPU</h2>
<div class="sectionbody">
<div class="paragraph text-justify">
<p>Tensor Processing Units (TPUs) are specialized ASIC chips designed to accelerate the performance of machine learning algorithms. They can be used in a variety of applications, ranging from cloud computing and edge computing to machine learning. TPUs provide an efficient way to process data, making them suitable for a range of tasks such as image recognition, language processing, and speech recognition. By leveraging the power of TPUs, organizations can reduce costs and optimize their operations.</p>
</div>
<div class="paragraph text-justify">
<p><strong>Cloud Computing:</strong> TPUs are used in cloud computing to provide better performance for workloads that require a lot of data processing. This allows businesses to process large amounts of data quickly and accurately at a lower cost than ever before. With the help of TPUs, businesses can make more informed decisions faster and improve their operational efficiency.</p>
</div>
<div class="paragraph text-justify">
<p><strong>Edge Computing:</strong> TPUs are also used in edge computing applications, which involve processing data at or near the source. This helps to reduce latency and improve performance for tasks such as streaming audio or video, autonomous driving, robotic navigation, and predictive analytics. Edge computing also facilitates faster and more reliable communication between devices in an IoT network.</p>
</div>
<div class="paragraph text-justify">
<p><strong>Machine Learning:</strong> TPUs are used to accelerate machine learning models and algorithms. They can be used to develop novel architectures that are optimized for tasks such as natural language processing, image recognition, and speech recognition. By leveraging the power of TPUs organizations can develop more complex models and algorithms faster. This will enable them to achieve better results with their machine-learning applications.</p>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer" style="border-top: 2px solid #e9e9e9; background-color: #fafafa; padding-bottom: 2em; padding-top: 2em;">
    <div class="container" style="display: flex; flex-direction: column; align-items: center; gap: 0.5em;">
        <div>
            <a href="https://www.cemosis.fr">
                <img src="../_/img/cemosis-logo.svg" alt="Cemosis logo" height="50">
            </a>
        </div>
        <span style="font-size: 0.8rem; color: #9e9e9e">Â© 2023 <a href="https://www.cemosis.fr" style="text-decoration: underline;">Cemosis</a>, UniversitÃ© de Strasbourg</span>
    </div>
</footer>
<script id="site-script" src="../_/js/site.js" data-ui-root-path="../_"></script>


<script async src="../_/js/vendor/fontawesome-icon-defs.js"></script>
<script async src="../_/js/vendor/fontawesome.js"></script>
<script async src="../_/js/vendor/highlight.js"></script>


<script type="text/javascript">
function toggleFullScreen() {
   var doc = window.document;
   var docEl = doc.documentElement;

   var requestFullScreen = docEl.requestFullscreen || docEl.mozRequestFullScreen || docEl.webkitRequestFullScreen || docEl.msRequestFullscreen;
   var cancelFullScreen = doc.exitFullscreen || doc.mozCancelFullScreen || doc.webkitExitFullscreen || doc.msExitFullscreen;

   if(!doc.fullscreenElement && !doc.mozFullScreenElement && !doc.webkitFullscreenElement && !doc.msFullscreenElement) {
       requestFullScreen.call(docEl);
   }
   else {
       cancelFullScreen.call(doc);
   }
}
</script>
  </body>
</html>
