<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>MPI (Message Passing Interface) :: Parallel Programming</title>
    <link rel="canonical" href="https://feelpp.github.io/parallel-programming/parallel-programming/PPChapter2_MPI.html">
    <meta name="generator" content="Antora 3.1.10">
    <link rel="stylesheet" href="../_/css/site.css">
<link rel="icon" href="../_/img/favicon.ico" type="image/x-icon">
<script>!function(l,p){if(l.protocol!==p&&l.host=="docs.antora.org"){l.protocol=p}else if(/\.gitlab\.io$/.test(l.host)){l.replace(p+"//docs.antora.org"+l.pathname.substr(l.pathname.indexOf("/",1))+l.search+l.hash)}}(location,"https:")</script>

<script src="../_/js/vendor/tabs-block-extension.js"></script>
<script src="../_/js/vendor/tabs-block-behavior.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },

  TeX: {
      Macros: {
      bold: ["{\\bf #1}",1],
      calTh: "{\\mathcal{T}_h}",
      card: ["{\\operatorname{card}(#1)}",1],
      card: ["{\\operatorname{card}(#1)}",1],
      Ck: ["{\\mathcal{C}^{#1}}",1],
      deformt: ["{\\mathbf{\\varepsilon(#1)}}",1],
      diam: "{\\operatorname{diam}}",
      dim: ["{\\operatorname{dim}(#1)}",1],
      disp: ["{\\mathbf{#1}}",1],
      domain: "{\\Omega}",
      ds: "",
      essinf: "{\\operatorname{ess}\\, \\operatorname{inf}}",
      F:"{\\mathcal{F}}",
      geo: "{\\mathrm{geo}}",
      Ich: ["{\\mathcal{I}^{#1}_{c,h}#2}",2],
      Id: "{\\mathcal{I}}",
      Ilag: ["{\\mathcal{I}^{\\mathrm{lag}}_{#1}}",1],
      jump: ["{[\\![ #1 ]\\!]}",1],
      n:"{\\mathbf{n}}",
      Ne: "{N_{\\mathrm{e}}}",
      Next: "{\\mathrm{n}}",
      nf: "{n_f}",
      ngeo: "{n_{\\mathrm{geo}}}",
      Nma: "{N_{\\mathrm{ma}}}",
      NN: "{\\mathbb N}",
      Nno: "{N_{\\mathrm{no}}}",
      Nso: "{N_{\\mathrm{so}}}",
      opdim: "{\\operatorname{dim}}",
      p: "{\\mathrm{p}}",
      P:"{\\mathcal{P}}",
      Pch: ["{P^{#1}_{c,h}}",1],
      Pcho: ["{P^{#1}_{c,h,0}}",1],
      Pk: ["{\\mathcal{P}^{#1}}",1],
      poly: ["{\\mathbb{#1}",1],
      poly: ["{\\mathbb{#1}}",1],
      prect: ["{\\left\\(#1\\right\\)}",1],
      q:"{\\mathbf{q}}",
      Qch: ["{Q^{#1}_{c,h}}",1],
      Qk: ["{\\mathcal{Q}^{#1}}",1],
      R: ["{\\mathbb{R}^{#1}}",1],
      RR: "{\\mathbb R}",
      set: ["{\\left\\{#1\\right\\}}",1],
      stresst: ["{\\mathbf{\\sigma(#1)}}",1],
      T:"{\\mathcal{T}}",
      tr: "{\\operatorname{tr}}",
      v:"{\\mathbf{v}}",
      vertiii: ["\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert #1 \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert",1]
  },
  extensions: ["mhchem.js"] 
  }
});
</script>
<!--<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>-->
<!-- <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML'></script>
<!--<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.0/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>-->

<!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>-->
<script>var uiRootPath = '../_'</script>

  </head>
  <body class="article">
<header class="header">
    <nav class="navbar navbar-expand-sm bg-dark navbar-dark navbar-template-project" style="border-top: 4px solid #9E9E9E">
        <div class="navbar-brand">
            <div class="navbar-item feelpp-logo">
                <a href="https://feelpp.github.io/parallel-programming">Parallel Programming</a>
            </div>
            <button class="navbar-burger" data-target="topbar-nav">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>

        <div id="topbar-nav" class="navbar-menu">
            <div class="navbar-end">
                <div class="navbar-item">
                    <a href="https://docs.feelpp.org/">Documentation Reference</a>
                </div>
                <div class="navbar-item has-dropdown is-hoverable download-item">
                    <div class="navbar-item"><a href="https://docs.feelpp.org/user/latest/install/index.html" class="download-btn">Get Feel++</a></div>
                </div>
                <div class="navbar-item">
                    <a class="navbar-brand"  href="https://www.cemosis.fr">
                        <img class="cemosis-logo"  src="../_/img/cemosis-logo.svg" alt="Cemosis logo"/>
                    </a>
                </div>
            </div>
        </div>
    </nav>
</header>
<div class="body">
<a href="#" class="menu-expand-toggle"></a>
<div class="nav-container" data-component="parallel-programming" data-version="">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="index.html">Main</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="index.html">Introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Architectures</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="architectures/PPChapter1_CPU.html">CPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="architectures/PPChapter1_GPU.html">GPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="architectures/PPChapter1_GPGPU.html">GPGPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="architectures/PPChapter1_TPU.html">TPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="architectures/PPChapter1_NPU.html">NPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="architectures/PPChapter1_LPU.html">LPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="architectures/PPChapter1_DPU.html">DPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="architectures/PPChapter1_SIMD.html">SIMD Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="architectures/PPChapter1_AMD_CUDA.html">AMD CUDA Architecture</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">MPI/OpenMP/Hybrid</span>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="PPChapter2_MPI.html">MPI (Message Passing Interface)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="PPChapter2_MPI_Boost.html">MPI Boost</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="PPChapter2_OpenMP.html">OpenMP (Open Multi-Processing)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="PPChapter2_OpenMP2.html">OpenMP more information</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="PPChapter2_Hybrid.html">Hybrid MPI with OpenMP</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Runtime Systems</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="PPChapter3.html">StarPU</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="PPChapter4.html">Specx</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="kokkos/index.html">Introduction</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="kokkos/introduction/why-kokkos.html">Why Kokkos?</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="kokkos/introduction/installation.html">Installation &amp; Setup</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="kokkos/basic-concepts/index.html">Basic Concepts</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="kokkos/basic-concepts/execution-spaces.html">Execution Spaces</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="kokkos/basic-concepts/memory-spaces.html">Memory Spaces</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="kokkos/basic-concepts/views.html">Views</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Main</span>
    <span class="version"></span>
  </div>
  <ul class="components">
      <li class="component">
        <a class="title" href="../feelpp-antora-ui/index.html">Antora Feel++ UI</a>
      </li>
      <li class="component is-current">
        <a class="title" href="index.html">Main</a>
      </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
  <button class="nav-toggle"></button>
    <a href="index.html" class="home-link"></a>
  <nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="index.html">Main</a></li>
    <li>MPI/OpenMP/Hybrid</li>
    <li><a href="PPChapter2_MPI.html">MPI (Message Passing Interface)</a></li>
  </ul>
</nav>

  
    <div class="edit-this-page"><a href="https://github.com/feelpp/course-parallel-programming/edit/master/docs/modules/ROOT/pages/PPChapter2_MPI.adoc">Edit this Page</a></div>
  
  <div class="page-downloads">
  <span class="label">Download as</span>
  <ul class="download-options">
    <li>
      <a onclick="print(this)" href="#" data-toggle="tooltip" data-placement="left" title="Print to PDF"
         class="pdf-download">
        <img class="pdf-file-icon icon" src="../_/img/pdf.svg"/> .pdf
      </a>
    </li>
  </ul>
</div>
</div>

  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="3">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">MPI (Message Passing Interface)</h1>
<div class="sect1">
<h2 id="_general_notions"><a class="anchor" href="#_general_notions"></a>1. General notions</h2>
<div class="sectionbody">
<div class="paragraph text-justify">
<p>The transmitter and the receiver are identified by their rank in the communicator. The entity passed between two processes is called a message. A message is characterized by its envelope.</p>
</div>
<div class="paragraph">
<p>This consists of:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of the sending process.</p>
</li>
<li>
<p>The rank of the receiving process.</p>
</li>
<li>
<p>The label ( <em>tag</em> ) of the message.</p>
</li>
<li>
<p>The communicator who defines the process group and the communication
context.</p>
<div class="paragraph text-justify">
<p>The data exchanged is typed (integers, reals, etc. or personal derived types) In each case, there are several transfer modes , using different
protocols.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Send( *const void* *message, *int* length, MPI_Datatyp type_message, *int* rank_dest, *int* label, MPI_Comm comm)</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Recv ( *void* *message, *int* length, MPI_Datatype type_message *int* rank_source, *int* label, MPI_Comm comm, MPI_Status *status)</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>And simultaneous send and receive operation:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Sendrecv_replace ( void * message, int length, MPI_Datatype type_message, int rank_dest, int label_message_sent, int* rank_source, int label_message_recu, MPI_Comm comm, MPI_Status *status).</pre>
</div>
</div>
<div class="paragraph">
<p>Note this operation is blocking.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_collective_communications"><a class="anchor" href="#_collective_communications"></a>2. Collective communications</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_general_notions_2"><a class="anchor" href="#_general_notions_2"></a>2.1. General notions</h3>
<div class="paragraph text-justify">
<p>Collective communications allow a series of point-to-point communications to be made in a single operation. A collective communication always concerns all the processes of the indicated communicator. For each of the processes, the call ends when the latter&#8217;s participation in the collective operation is completed, in the sense of point-to-point communications (thus when the memory zone concerned can be modified). The management of labels in these communications is transparent and at the expense of the system. They are therefore never explicitly defined during the call to these subroutines. One of the advantages of this is that collective communications never interfere with point-to-point communications.</p>
</div>
</div>
<div class="sect2">
<h3 id="_types_of_collective_communications"><a class="anchor" href="#_types_of_collective_communications"></a>2.2. Types of collective communications</h3>
<div class="paragraph text-justify">
<p>There are three types of subroutines:
* The one that ensures global synchronizations: MPI_Barrier().</p>
</div>
<div class="ulist">
<ul>
<li>
<p>those that only transfer data:</p>
<div class="ulist">
<ul>
<li>
<p>global data broadcasting: MPI_Bcast();</p>
</li>
<li>
<p>selective diffusion of data: MPI_Scatter();</p>
</li>
<li>
<p>distributed data collection: MPI_Gather();</p>
</li>
<li>
<p>collection by all distributed data processes: MPI_Allgather();</p>
</li>
<li>
<p>selective collection and dissemination, by all processes, of distributed data: MPI_Alltoall().</p>
</li>
</ul>
</div>
</li>
<li>
<p>those who, in addition to managing communications, perform
operations on the transferred data:</p>
<div class="ulist">
<ul>
<li>
<p>reduction operations (sum, product, maximum, minimum, etc.), whether of
a predefined type or of a personal type: MPI_Reduce();</p>
</li>
<li>
<p>reduction operations with distribution of the result (equivalent to an
MPI_Reduce() followed by an MPI_Bcast()): MPI_Allreduce().</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_global_synchronization"><a class="anchor" href="#_global_synchronization"></a>3. Global synchronization</h2>
<div class="sectionbody">
<div class="literalblock">
<div class="content">
<pre>int MPI_Barrier ( MPI_Comm comm)</pre>
</div>
</div>
<div class="paragraph">
<p><strong>General distribution</strong></p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Bcast( void *message, int length, MPI_Datatype, type_message, *int* rank_source, MPI_Comm comm)</pre>
</div>
</div>
<div class="paragraph">
<p><strong>Selective dissemination</strong></p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Scatter ( const void *message_to_be restarted, int length_message_sent, MPI_Datatype type_message_sent, void *message_received, int length_message_recu, MPI_Datatype type_message_recu, int rank_source, MPI_Comm comm)</pre>
</div>
</div>
<div class="paragraph">
<p><strong>Collection</strong></p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Gather ( const void *message_sent, int length_message_sent, MPI_Datatype type_message_sent, void *message_received, int length_message_received, MPI_Datatype type_message_received, *int* rank_dest, MPI_Comm comm)</pre>
</div>
</div>
<div class="paragraph">
<p><strong>General collection</strong></p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Allgather ( const void *message_sent, int length_message_sent, MPI_Datatype type_message_sent, void *message_received, int length_message_received, MPI_Datatype type_message_received, MPI_Comm comm)</pre>
</div>
</div>
<div class="paragraph">
<p><strong>"Variable" collection</strong></p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Gatherv ( const void *message_sent, int length_message_sent, MPI_Datatype type_message_sent, void *message_received, const int *nb_elts_recus, const int *deplts, MPI_Datatype type_message_recu, *int* rang_dest, MPI_Comm comm)</pre>
</div>
</div>
<div class="paragraph">
<p><strong>Selective collections and distributions</strong></p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Alltoall ( const void *message_sent, int length_message_sent, MPI_Datatype type_message_sent, void *message_received, int length_message_received, MPI_Datatype type_message_received, MPI_Comm comm)</pre>
</div>
</div>
<div class="paragraph">
<p><strong>Distributed reductions</strong></p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Reduce ( const void *message_sent, void *message_received, int length, MPI_Datatype type_message, MPI_Op operation, int rank_dest,* MPI_Comm comm)</pre>
</div>
</div>
<div class="paragraph">
<p><strong>Distributed reductions with distribution of the result</strong></p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Allreduce ( const void *message_sent, void *message_received, *int length, MPI_Datatype, type_message, MPI_Op operation, MPI_Comm comm)</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_communication_models"><a class="anchor" href="#_communication_models"></a>4. Communication models</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_point_to_point_sending_modes"><a class="anchor" href="#_point_to_point_sending_modes"></a>4.1. Point-to-point sending modes</h3>
<div class="ulist">
<ul>
<li>
<p>Blocking and Non-blocking mode</p>
</li>
<li>
<p>Standard sending MPI_Send() MPI_Isend()</p>
</li>
<li>
<p>Synchronous send MPI_Ssend() MPI_Issend()</p>
</li>
<li>
<p><em>Buffered</em> send MPI_Bsend() MPI_Ibsend()</p>
</li>
<li>
<p>Receive MPI_Recv() MPI_Irecv()
A call is blocking if the memory space used for communication can be reused immediately after the call exits. The data sent can be modified after the blocking call. The received data can be read after the blocking call.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_synchronous_sends"><a class="anchor" href="#_synchronous_sends"></a>4.2. Synchronous sends</h3>
<div class="paragraph text-justify">
<p>A synchronous send involves synchronization between the processes involved. A shipment can only begin when its receipt is posted. There
can only be communication if both processes are willing to communicate.</p>
</div>
<div class="paragraph">
<p><strong>int</strong> MPI_Ssend( <strong>const void</strong> * values, <strong>int</strong> size, MPI_Datatype
message_type, <strong>int</strong> dest, <strong>int</strong> label, MPI_Comm comm)</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Benefits</p>
<div class="ulist">
<ul>
<li>
<p>Consume few resources (no <em>buffer</em> ) Fast if the receiver is ready (no copying into a <em>buffer</em> ) Recognition of reception thanks to synchronization</p>
</li>
</ul>
</div>
</li>
<li>
<p>Disadvantages</p>
<div class="ulist">
<ul>
<li>
<p>Waiting time if the receiver is not there/not ready Risks of deadlock</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_buffered_sends"><a class="anchor" href="#_buffered_sends"></a>4.3. Buffered sends</h3>
<div class="paragraph text-justify">
<p>A buffered send involves the copying of data into an intermediate memory space. There is then no coupling between the two communication processes. The output of this type of sending therefore does not mean that the reception has taken place.</p>
</div>
<div class="paragraph text-justify">
<p>Buffers must be managed manually (with calls to MPI_Buffer_attach( <em>)</em>
and MPI_Buffer_detach()). They must be allocated taking into account the
memory overhead of the messages (by adding the MPI_BSEND_OVERHEAD
constant for each message instance).</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Buffer_attach ( void *buf, int size_buf)
int MPI_Buffer_detach ( void *buf, int size_buf)
int MPI_Bsend( const void *values, int size, MPI_Datatype type_message, int dest, int label, MPI_Comm comm)</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Advantages of buffered mode</p>
<div class="ulist">
<ul>
<li>
<p>No need to wait for the receiver (recopy in a <em>buffer</em> ) No risk of
blocking ( <em>deadlocks</em> )</p>
</li>
</ul>
</div>
</li>
<li>
<p>Disadvantages of buffered mode</p>
<div class="ulist">
<ul>
<li>
<p>Consume more resources (memory occupation by <em>buffers</em> with risk of
saturation)</p>
</li>
<li>
<p>Send buffers must be managed manually (often difficult to choose an
appropriate size <em>)</em></p>
</li>
<li>
<p>A bit slower than synchronous sends if the receiver is ready</p>
</li>
<li>
<p>No knowledge of the reception (send-receive decoupling)</p>
</li>
<li>
<p>Risk of wasting memory space if the <em>buffers</em> are too oversized</p>
</li>
<li>
<p>The application crashes if the <em>buffers</em> are too small</p>
</li>
<li>
<p>There are also often hidden <em>buffers</em> managed by the MPI implementation
on the sender and/or receiver side (and consuming memory resources)</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_non_blocking_calls"><a class="anchor" href="#_non_blocking_calls"></a>5. Non-blocking calls</h2>
<div class="sectionbody">
<div class="paragraph text-justify">
<p><strong>Non-blocking</strong> call returns control very quickly, but does not allow the
immediate reuse of the memory space used in the call. It is necessary to
ensure that the communication is indeed terminated (with MPI_Wait() for
example) before using it again.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Isend( const void *values, int size, MPI_Datatype
message_type, int dest, int label, MPI_Comm comm, MPI_Request *req)</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Issend ( const void* values, int size, MPI_Datatype
message_type, int dest, int label, MPI_Comm comm, MPI_Request *req)</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Ibsend( const void* values, int size, MPI_Datatype
message_type, int dest, int label, MPI_Comm comm, MPI_Request *req)</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Irecv( void *values, int size, MPI_Datatype type_message,
int* source, int label, MPI_Comm comm, MPI_Request *req)</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Benefits of non-blocking calls</p>
<div class="ulist text-justify">
<ul>
<li>
<p>Ability to hide all or part of the communication costs (if the architecture allows it). No risk of <em>deadlock</em>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Disadvantages of non-blocking calls</p>
<div class="ulist text-justify">
<ul>
<li>
<p>Higher additional costs (several calls for a single send or receive, request management).</p>
</li>
<li>
<p>Higher complexity and more complicated maintenance.</p>
</li>
<li>
<p>Risk of loss of performance on the calculation cores (for example differentiated management between the zone close to the border of a domain and the interior zone resulting in less good use of memory caches).</p>
</li>
<li>
<p>Limited to point-to-point communications.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_memory_to_memory_communications"><a class="anchor" href="#_memory_to_memory_communications"></a>6. Memory to memory communications</h2>
<div class="sectionbody">
<div class="paragraph text-justify">
<p>Memory-to-memory communications (or RMA for <em>Remote Memory Access</em> or <em>one-sided communications</em> ) consist of accessing the memory of a remote process in write or read mode without the latter having to manage this access explicitly. The target process therefore does not intervene during the transfer.</p>
</div>
<div class="sect2">
<h3 id="_rma_general_approach"><a class="anchor" href="#_rma_general_approach"></a>6.1. RMA - General Approach</h3>
<div class="paragraph text-justify">
<p>Creation of a memory window with MPI_Win_create() to authorize RMA transfers in this area.</p>
</div>
<div class="paragraph">
<p>Remote read or write access by calling MPI_Put(), MPI_Get(), MPI_Accumulate(), MPI_Get_accumulate() and MPI_Compare_and_swap()</p>
</div>
<div class="paragraph">
<p>Freeing the memory window with M PI_Win_free().</p>
</div>
</div>
<div class="sect2">
<h3 id="_rma_synchronization_methods"><a class="anchor" href="#_rma_synchronization_methods"></a>6.2. RMA - Synchronization Methods</h3>
<div class="paragraph text-justify">
<p>To ensure correct operation, it is mandatory to carry out certain synchronizations. 3 methods are available:</p>
</div>
<div class="paragraph">
<p>Active target communication with global synchronization (MPI_Win_fence() );</p>
</div>
<div class="paragraph">
<p>Communication with active target with pair synchronization
(MPI_Win_start() and MPI_Win_complete() for the origin process;
MPI_Win-post() and MPI_Win_wait() for the target process);</p>
</div>
<div class="paragraph">
<p>Passive target communication without target intervention (MPI_Win_lock()
and MPI_Win_unlock()).</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Benefits of RMAs</p>
<div class="ulist text-justify">
<ul>
<li>
<p>Allows you to implement certain algorithms more efficiently.</p>
</li>
<li>
<p>More efficient than point-to-point communications on some machines (use of specialized hardware such as DMA engine, coprocessor, specialized memory, etc.).</p>
</li>
<li>
<p>Ability for the implementation to group multiple operations.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Disadvantages of RMAs</p>
<div class="ulist text-justify">
<ul>
<li>
<p>Synchronization management is tricky.</p>
</li>
<li>
<p>Complexity and high risk of error.</p>
</li>
<li>
<p>For passive target synchronizations, obligation to allocate memory with
MPI_Alloc_mem() which does not respect the Fortran standard (use of Cray
pointers not supported by some compilers).</p>
</li>
<li>
<p>Less efficient than point-to-point communications on some machines.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_interfaces"><a class="anchor" href="#_interfaces"></a>7. Interfaces</h2>
<div class="sectionbody">
<div class="paragraph text-justify">
<p>MPI_Wait() waits for the end of a communication. MPI_Test() is the
non-blocking version.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Wait ( MPI_Request *req, MPI_Status *status)
int MPI_Test( MPI_Request *req, int *flag, MPI_Status *status)</pre>
</div>
</div>
<div class="paragraph text-justify">
<p>MPI_Waitall() waits for all communications to end. MPI_Testall() is the
non-blocking version.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Waitall ( int size, MPI_Request reqs[], MPI_Status statuses[])
int* MPI_Testall ( int size, MPI_Request reqs[], int *flag, MPI_Status statuses[])</pre>
</div>
</div>
<div class="paragraph text-justify">
<p>MPI_Waitany waits for the end of one communication among several.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Waitany ( int size, MPI_Request reqs[], int *index,MPI_Status *status)</pre>
</div>
</div>
<div class="paragraph">
<p>MPI_Testany is the non-blocking version.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int* MPI_Testany( int size, MPI_Request reqs[], int *index, int *flag, MPI_Status *status)</pre>
</div>
</div>
<div class="paragraph">
<p>MPI_Waitsome is waiting for the end of one or more communications.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Waitsome( int size, MPI_Request reqs[], int *endcount,int *indexes, MPI_Status *status)</pre>
</div>
</div>
<div class="paragraph">
<p>MPI_Testsome is the non-blocking version.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Testsome( int size, MPI_Request reqs[], int *endcount,int *indexes, MPI_Status *status)</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_mpi_keywords"><a class="anchor" href="#_mpi_keywords"></a>8. MPI keywords</h2>
<div class="sectionbody">
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p><strong>1 environment</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>MPI Init: Initialization of the MPI environment</p>
</li>
<li>
<p>MPI Comm rank: Rank of the process</p>
</li>
<li>
<p>MPI Comm size: Number of processes</p>
</li>
<li>
<p>MPI Finalize: Deactivation of the MPI environment</p>
</li>
<li>
<p>MPI Abort:Stopping of an MPI program</p>
</li>
<li>
<p>MPI Wtime: Time taking</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>2 Point-to-point communications</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>MPI Send: Send message</p>
</li>
<li>
<p>MPI Isend: Non-blocking message sending</p>
</li>
<li>
<p>MPI Recv: Message received</p>
</li>
<li>
<p>MPI Irecv: Non-blocking message reception</p>
</li>
<li>
<p>MPI Sendrecv and MPI Sendrecv replace: Sending and receiving messages</p>
</li>
<li>
<p>MPI Wait: Waiting for the end of a non-blocking communication</p>
</li>
<li>
<p>MPI Wait all: Wait for the end of all non-blocking communications</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>3 Collective communications</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>MPI Bcast: General broadcast</p>
</li>
<li>
<p>MPI Scatter: Selective spread</p>
</li>
<li>
<p>MPI Gather and MPI Allgather: Collecting</p>
</li>
<li>
<p>MPI Alltoall: Collection and distribution</p>
</li>
<li>
<p>MPI Reduce and MPI Allreduce: Reduction</p>
</li>
<li>
<p>MPI Barrier: Global synchronization</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>4 Derived Types</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>MPI Contiguous type: Contiguous types</p>
</li>
<li>
<p>MPI Type vector and MPI Type create hvector: Types with a con-standing</p>
</li>
<li>
<p>MPI Type indexed: Variable pitch types</p>
</li>
<li>
<p>MPI Type create subarray: Sub-array types</p>
</li>
<li>
<p>MPI Type create struct: H and erogenous types</p>
</li>
<li>
<p>MPI Type commit: Type commit</p>
</li>
<li>
<p>MPI Type get extent: Recover the extent</p>
</li>
<li>
<p>MPI Type create resized: Change of scope</p>
</li>
<li>
<p>MPI Type size: Size of a type</p>
</li>
<li>
<p>MPI Type free: Release of a type</p>
</li>
</ul>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p><strong>5 Communicator</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>MPI Comm split: Partitioning of a communicator</p>
</li>
<li>
<p>MPI Dims create: Distribution of processes</p>
</li>
<li>
<p>MPI Cart create: Creation of a Cart ́esian topology</p>
</li>
<li>
<p>MPI Cart rank: Rank of a process in the Cart ́esian topology</p>
</li>
<li>
<p>MPI Cart coordinates: Coordinates of a process in the Cart ́esian
topology</p>
</li>
<li>
<p>MPI Cart shift: Rank of the neighbors in the Cart ́esian topology</p>
</li>
<li>
<p>MPI Comm free: Release of a communicator</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>6 MPI-IO</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>MPI File open: Opening a file</p>
</li>
<li>
<p>MPI File set view: Changing the view • MPI File close: Closing a file</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>6.1 Explicit addresses</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>MPI File read at: Reading</p>
</li>
<li>
<p>MPI File read at all: Collective reading</p>
</li>
<li>
<p>MPI File write at: Writing</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>6.2 Individual pointers</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>MPI File read: Reading</p>
</li>
<li>
<p>MPI File read all: collective reading</p>
</li>
<li>
<p>MPI File write: Writing</p>
</li>
<li>
<p>MPI File write all: collective writing</p>
</li>
<li>
<p>MPI File seek: Pointer positioning</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>6.3 Shared pointers</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>MPI File read shared: Read</p>
</li>
<li>
<p>MPI File read ordered: Collective reading</p>
</li>
<li>
<p>MPI File seek shared: Pointer positioning</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>7.0 Symbolic constants</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>MPI COMM WORLD, MPI SUCCESS</p>
</li>
<li>
<p>MPI STATUS IGNORE, MPI PROC NULL</p>
</li>
<li>
<p>MPI INTEGER, MPI REAL, MPI DOUBLE PRECISION</p>
</li>
<li>
<p>MPI ORDER FORTRAN, MPI ORDER C</p>
</li>
<li>
<p>MPI MODE CREATE,MPI MODE RONLY,MPI MODE WRONLY</p>
</li>
</ul>
</div></div></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1">
<h2 id="_derived_data_types"><a class="anchor" href="#_derived_data_types"></a>9. Derived data types</h2>
<div class="sectionbody">
<div class="paragraph text-justify">
<p>In the communications, the data exchanged are typed: MPI_INTEGER,
MPI_REAL, MPI_COMPLEX, etc .</p>
</div>
<div class="paragraph">
<p>More complex data structures can be created using subroutines such as
MPI_Type_contiguous(), MPI_Type_vector(), MPI_Type_Indexed() , or
MPI_Type_create_struct()</p>
</div>
<div class="paragraph text-justify">
<p>The derived types notably allow the exchange of non-contiguous or
non-homogeneous data in memory and to limit the number of calls to the
communications subroutines.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_mpi_threading"><a class="anchor" href="#_mpi_threading"></a>10. MPI + threading</h2>
<div class="sectionbody">
<div class="paragraph text-justify">
<p>The MPI standard has been updated to accommodate the use of threads within processes. Using these capabilities is optional, and presents numerous advantages and disadvantages</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Advantages of MPI + threading</p>
<div class="ulist text-justify">
<ul>
<li>
<p>Possiblity for better scaling of communication costs</p>
</li>
<li>
<p>Either simpler and/or faster code that does not need to distribute as much data, because all threads in the process can share it already</p>
</li>
<li>
<p>Higher performance from using memory caches better</p>
</li>
<li>
<p>Reduced need to dedicate a rank solely to communication coordination in code using a manager-worker paradigm</p>
</li>
</ul>
</div>
</li>
<li>
<p>Disadvantages of MPI + threading</p>
<div class="ulist text-justify">
<ul>
<li>
<p>Implicitly shared data can be harder to reason about correctly (eg. race conditions)</p>
</li>
<li>
<p>Code now has to be correct MPI code and correct threaded code</p>
</li>
<li>
<p>Possibility of lower performance from cache contention, when one thread writes to memory that is very close to where another thread needs to read</p>
</li>
<li>
<p>More code complexity</p>
</li>
<li>
<p>Might be merely shifting bottlenecks from one place to another (eg. opening and closing OpenMP thread regions)</p>
</li>
<li>
<p>Needs higher quality MPI implementations</p>
</li>
<li>
<p>It can be awkward to use libraries that also use threading internally</p>
</li>
<li>
<p>Usage gets more complicated, as both ranks and threads have to be shepherded onto cores for maximum performance</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_mpi_support_for_threading"><a class="anchor" href="#_mpi_support_for_threading"></a>11. MPI support for threading</h2>
<div class="sectionbody">
<div class="paragraph text-justify">
<p>Since version 2.0, MPI can be initialized in up to four different ways. The former approach using MPI_Init still works, but applications that wish to use threading should use MPI_Init_thread.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int MPI_Init_thread(int *argc, char ***argv, int required, int *provided)</pre>
</div>
</div>
<div class="paragraph text-justify">
<p>The following threading levels are generally supported:</p>
</div>
<div class="ulist text-justify">
<ul>
<li>
<p>MPI_THREAD_SINGLE - rank is not allowed to use threads, which is basically equivalent to calling MPI_Init.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><span class="image"><a class="image" href="#fragment1113"><img src="_images/MPI_THREAD_SINGLE.svg" alt="MPI THREAD SINGLE" width="600" height="300"></a></span></p>
</div>
<div class="literalblock">
<div class="content">
<pre>With MPI_THREAD_SINGLE, the rank may use MPI freely and will not use threads.</pre>
</div>
</div>
<div class="paragraph text-justify">
<p>*MPI_THREAD_FUNNELED - rank can be multi-threaded but only the main thread may call MPI functions. Ideal for fork-join parallelism such as used in #pragma omp parallel, where all MPI calls are outside the OpenMP regions.</p>
</div>
<div class="paragraph">
<p><span class="image"><a class="image" href="#fragment1114"><img src="_images/MPI_THREAD_FUNNELED.svg" alt="MPI THREAD FUNNELED" width="600" height="300"></a></span></p>
</div>
<div class="literalblock text-justify">
<div class="content">
<pre>With MPI_THREAD_FUNNELED, the rank can use MPI from only the main thread.</pre>
</div>
</div>
<div class="ulist text-justify">
<ul>
<li>
<p>MPI_THREAD_SERIALIZED - rank can be multi-threaded but only one thread at a time may call MPI functions. The rank must ensure that MPI is used in a thread-safe way. One approach is to ensure that MPI usage is mutually excluded by all the threads, eg. with a mutex.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><span class="image"><a class="image" href="#fragment1115"><img src="_images/MPI_THREAD_SERIALIZED.svg" alt="MPI THREAD SERIALIZED" width="600" height="300"></a></span></p>
</div>
<div class="literalblock text-justify">
<div class="content">
<pre>With MPI_THREAD_SERIALIZED, the rank can use MPI from any thread so long as it ensures the threads synchronize such that no thread calls MPI while another thread is doing so.</pre>
</div>
</div>
<div class="ulist text-justify">
<ul>
<li>
<p>MPI_THREAD_MULTIPLE - rank can be multi-threaded and any thread may call MPI functions. The MPI library ensures that this access is safe across threads. Note that this makes all MPI operations less efficient, even if only one thread makes MPI calls, so should be used only where necessary.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><span class="image"><a class="image" href="#fragment1116"><img src="_images/MPI_THREAD_MULTIPLE.svg" alt="MPI THREAD MULTIPLE" width="600" height="300"></a></span></p>
</div>
<div class="paragraph text-justify">
<p>With MPI_THREAD_MULTIPLE, the rank can use MPI from any thread. The MPI library ensures the necessary synchronization</p>
</div>
<div class="paragraph text-justify">
<p>Note that different MPI ranks may make different requirements for MPI threading. This can be efficient for applications using manager-worker paradigms where the workers have simpler communication patterns.</p>
</div>
<div class="paragraph text-justify">
<p>For applications where it is possible to implement using MPI_THREAD_SERIALIZED approach, it will generally outperform the same application naively implemented and using MPI_THREAD_MULTIPLE, because the latter will need to use more synchronization.</p>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer" style="border-top: 2px solid #e9e9e9; background-color: #fafafa; padding-bottom: 2em; padding-top: 2em;">
    <div class="container" style="display: flex; flex-direction: column; align-items: center; gap: 0.5em;">
        <div>
            <a href="https://www.cemosis.fr">
                <img src="../_/img/cemosis-logo.svg" alt="Cemosis logo" height="50">
            </a>
        </div>
        <span style="font-size: 0.8rem; color: #9e9e9e">© 2025 <a href="https://www.cemosis.fr" style="text-decoration: underline;">Cemosis</a>, Université de Strasbourg</span>
    </div>
</footer>
<script id="site-script" src="../_/js/site.js" data-ui-root-path="../_"></script>


<script async src="../_/js/vendor/fontawesome-icon-defs.js"></script>
<script async src="../_/js/vendor/fontawesome.js"></script>
<script async src="../_/js/vendor/highlight.js"></script>


<script type="text/javascript">
function toggleFullScreen() {
   var doc = window.document;
   var docEl = doc.documentElement;

   var requestFullScreen = docEl.requestFullscreen || docEl.mozRequestFullScreen || docEl.webkitRequestFullScreen || docEl.msRequestFullscreen;
   var cancelFullScreen = doc.exitFullscreen || doc.mozCancelFullScreen || doc.webkitExitFullscreen || doc.msExitFullscreen;

   if(!doc.fullscreenElement && !doc.mozFullScreenElement && !doc.webkitFullscreenElement && !doc.msFullscreenElement) {
       requestFullScreen.call(docEl);
   }
   else {
       cancelFullScreen.call(doc);
   }
}
</script>
  </body>
</html>
