<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>AMD ROCm Platform,CUDA :: Parallel Programming</title>
    <link rel="canonical" href="https://feelpp.github.io/parallel-programming/parallel-programming/architectures/PPChapter1_AMD_CUDA.html">
    <meta name="generator" content="Antora 3.1.10">
    <link rel="stylesheet" href="../../_/css/site.css">
<link rel="icon" href="../../_/img/favicon.ico" type="image/x-icon">
<script>!function(l,p){if(l.protocol!==p&&l.host=="docs.antora.org"){l.protocol=p}else if(/\.gitlab\.io$/.test(l.host)){l.replace(p+"//docs.antora.org"+l.pathname.substr(l.pathname.indexOf("/",1))+l.search+l.hash)}}(location,"https:")</script>

<script src="../../_/js/vendor/tabs-block-extension.js"></script>
<script src="../../_/js/vendor/tabs-block-behavior.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },

  TeX: {
      Macros: {
      bold: ["{\\bf #1}",1],
      calTh: "{\\mathcal{T}_h}",
      card: ["{\\operatorname{card}(#1)}",1],
      card: ["{\\operatorname{card}(#1)}",1],
      Ck: ["{\\mathcal{C}^{#1}}",1],
      deformt: ["{\\mathbf{\\varepsilon(#1)}}",1],
      diam: "{\\operatorname{diam}}",
      dim: ["{\\operatorname{dim}(#1)}",1],
      disp: ["{\\mathbf{#1}}",1],
      domain: "{\\Omega}",
      ds: "",
      essinf: "{\\operatorname{ess}\\, \\operatorname{inf}}",
      F:"{\\mathcal{F}}",
      geo: "{\\mathrm{geo}}",
      Ich: ["{\\mathcal{I}^{#1}_{c,h}#2}",2],
      Id: "{\\mathcal{I}}",
      Ilag: ["{\\mathcal{I}^{\\mathrm{lag}}_{#1}}",1],
      jump: ["{[\\![ #1 ]\\!]}",1],
      n:"{\\mathbf{n}}",
      Ne: "{N_{\\mathrm{e}}}",
      Next: "{\\mathrm{n}}",
      nf: "{n_f}",
      ngeo: "{n_{\\mathrm{geo}}}",
      Nma: "{N_{\\mathrm{ma}}}",
      NN: "{\\mathbb N}",
      Nno: "{N_{\\mathrm{no}}}",
      Nso: "{N_{\\mathrm{so}}}",
      opdim: "{\\operatorname{dim}}",
      p: "{\\mathrm{p}}",
      P:"{\\mathcal{P}}",
      Pch: ["{P^{#1}_{c,h}}",1],
      Pcho: ["{P^{#1}_{c,h,0}}",1],
      Pk: ["{\\mathcal{P}^{#1}}",1],
      poly: ["{\\mathbb{#1}",1],
      poly: ["{\\mathbb{#1}}",1],
      prect: ["{\\left\\(#1\\right\\)}",1],
      q:"{\\mathbf{q}}",
      Qch: ["{Q^{#1}_{c,h}}",1],
      Qk: ["{\\mathcal{Q}^{#1}}",1],
      R: ["{\\mathbb{R}^{#1}}",1],
      RR: "{\\mathbb R}",
      set: ["{\\left\\{#1\\right\\}}",1],
      stresst: ["{\\mathbf{\\sigma(#1)}}",1],
      T:"{\\mathcal{T}}",
      tr: "{\\operatorname{tr}}",
      v:"{\\mathbf{v}}",
      vertiii: ["\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert #1 \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert",1]
  },
  extensions: ["mhchem.js"] 
  }
});
</script>
<!--<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>-->
<!-- <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML'></script>
<!--<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.0/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>-->

<!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>-->
<script>var uiRootPath = '../../_'</script>

  </head>
  <body class="article">
<header class="header">
    <nav class="navbar navbar-expand-sm bg-dark navbar-dark navbar-template-project" style="border-top: 4px solid #9E9E9E">
        <div class="navbar-brand">
            <div class="navbar-item feelpp-logo">
                <a href="https://feelpp.github.io/parallel-programming">Parallel Programming</a>
            </div>
            <button class="navbar-burger" data-target="topbar-nav">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>

        <div id="topbar-nav" class="navbar-menu">
            <div class="navbar-end">
                <div class="navbar-item">
                    <a href="https://docs.feelpp.org/">Documentation Reference</a>
                </div>
                <div class="navbar-item has-dropdown is-hoverable download-item">
                    <div class="navbar-item"><a href="https://docs.feelpp.org/user/latest/install/index.html" class="download-btn">Get Feel++</a></div>
                </div>
                <div class="navbar-item">
                    <a class="navbar-brand"  href="https://www.cemosis.fr">
                        <img class="cemosis-logo"  src="../../_/img/cemosis-logo.svg" alt="Cemosis logo"/>
                    </a>
                </div>
            </div>
        </div>
    </nav>
</header>
<div class="body">
<a href="#" class="menu-expand-toggle"></a>
<div class="nav-container" data-component="parallel-programming" data-version="">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Main</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Architectures</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="PPChapter1_CPU.html">CPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="PPChapter1_GPU.html">GPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="PPChapter1_GPGPU.html">GPGPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="PPChapter1_TPU.html">TPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="PPChapter1_NPU.html">NPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="PPChapter1_LPU.html">LPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="PPChapter1_DPU.html">DPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="PPChapter1_SIMD.html">SIMD Architecture</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="PPChapter1_AMD_CUDA.html">AMD CUDA Architecture</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">MPI/OpenMP/Hybrid</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../PPChapter2_MPI.html">MPI (Message Passing Interface)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../PPChapter2_MPI_Boost.html">MPI Boost</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../PPChapter2_OpenMP.html">OpenMP (Open Multi-Processing)</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../PPChapter2_OpenMP2.html">OpenMP more information</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../PPChapter2_Hybrid.html">Hybrid MPI with OpenMP</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../PPChapter3.html">StarPU</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../PPChapter4.html">Specx</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../kokkos/index.html">Introduction</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../kokkos/introduction/why-kokkos.html">Why Kokkos?</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../kokkos/introduction/installation.html">Installation &amp; Setup</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../kokkos/basic-concepts/index.html">Basic Concepts</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../kokkos/basic-concepts/execution-spaces.html">Execution Spaces</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../kokkos/basic-concepts/memory-spaces.html">Memory Spaces</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../kokkos/basic-concepts/views.html">Views</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Main</span>
    <span class="version"></span>
  </div>
  <ul class="components">
      <li class="component">
        <a class="title" href="../../feelpp-antora-ui/index.html">Antora Feel++ UI</a>
      </li>
      <li class="component is-current">
        <a class="title" href="../index.html">Main</a>
      </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
  <button class="nav-toggle"></button>
    <a href="../index.html" class="home-link"></a>
  <nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Main</a></li>
    <li>Architectures</li>
    <li><a href="PPChapter1_AMD_CUDA.html">AMD CUDA Architecture</a></li>
  </ul>
</nav>

  
    <div class="edit-this-page"><a href="https://github.com/feelpp/course-parallel-programming/edit/master/docs/modules/ROOT/pages/architectures/PPChapter1_AMD_CUDA.adoc">Edit this Page</a></div>
  
  <div class="page-downloads">
  <span class="label">Download as</span>
  <ul class="download-options">
    <li>
      <a onclick="print(this)" href="#" data-toggle="tooltip" data-placement="left" title="Print to PDF"
         class="pdf-download">
        <img class="pdf-file-icon icon" src="../../_/img/pdf.svg"/> .pdf
      </a>
    </li>
  </ul>
</div>
</div>

  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="3">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">AMD ROCm Platform,CUDA</h1>
<div class="sect1 text-justify">
<h2 id="_amd_roc_platform"><a class="anchor" href="#_amd_roc_platform"></a>1. AMD ROC platform</h2>
<div class="sectionbody">
<div class="paragraph">
<p>ROCm™ is a collection of drivers , development tools, and APIs that enable GPU programming from low-level kernel to end-user applications. ROCm is powered by AMD&#8217;s Heterogeneous Computing Interface for Portability , an OSS C++ GPU programming environment and its corresponding runtime environment. HIP enables ROCm developers to build portable applications across different platforms by deploying code on a range of platforms , from dedicated gaming GPUs to exascale HPC clusters. ROCm supports programming models such as OpenMP and OpenCL , and
includes all necessary compilers , debuggers and OSS libraries. ROCm is fully integrated with ML frameworks such as PyTorch and TensorFlow .ROCm can be deployed in several ways , including through the use of containers such as Docker,Spack, and your own build from source.</p>
</div>
<div class="paragraph">
<p>ROCm is designed to help develop,test,and deploy GPU-accelerated HPC,AI,scientific computing, CAD, and other applications in a free, open-source,integrated, and secure software ecosystem.</p>
</div>
<div class="paragraph">
<p><strong>CUDA Platform</strong></p>
</div>
<div class="paragraph text-justify">
<p>CUDA® is a parallel computing platform and programming model developed by NVIDIA for general computing on graphics processing units (GPUs). With CUDA, developers can dramatically speed up computing applications by harnessing the power of GPUs.</p>
</div>
<div class="paragraph text-justify">
<p>The CUDA architecture is based on a three-level hierarchy of cores, threads, and blocks. Cores are the basic unit of computation while threads are the individual pieces of work that the cores work on. Blocks are collections of threads that are grouped together and can be run together. This architecture enables efficient use of GPU resources and makes it possible to run multiple applications at once.</p>
</div>
<div class="paragraph text-justify">
<p>The NVIDIA CUDA-X platform, which is built on CUDA®, brings together a collection of libraries, tools, and technologies that deliver significantly higher performance than competing solutions in multiple application areas ranging from artificial intelligence to high performance computing.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"><strong>GPUs</strong></th>
<th class="tableblock halign-left valign-top"></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>CUDA ( Compute Unified Device Architecture)</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>HIP
("Heterogeneous-Compute Interface for Portability")</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Has been the de facto standard for native GPU code for years
Huge set of optimized libraries available
Custom syntax (extension of C++) supported only by CUDA compilers
Support for NVIDIA devices only</p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>AMD&#8217;s effort to offer a common programming interface that works on both CUDA and ROCm devices Standard C++ syntax, uses the nvcc/hcc compiler in the background
Almost an individual CUDA clone from the user&#8217;s perspective
The ecosystem is new and growing rapidly</p>
</div></div></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p><strong>1.5.3 What is the difference between CUDA and ROCm for GPGPU applications?</strong></p>
</div>
<div class="paragraph">
<p>NVIDIA&#8217;s CUDA and AMD&#8217;s ROCm provide frameworks to take advantage of the respective GPU platforms.</p>
</div>
<div class="paragraph text-justify">
<p>Graphics processing units (GPUs) are traditionally designed to handle graphics computing tasks, such as image and video processing and rendering, 2D and 3D graphics, vectorization, etc. General purpose computing on GPUs became more practical and popular after 2001, with the advent of programmable shaders and floating point support on graphics processors.</p>
</div>
<div class="paragraph text-justify">
<p>Notably, it involved problems with matrices and vectors, including two-, three-, or four-dimensional vectors. These were easily translated to GPU, which acts with native speed and support on these types. A milestone for general purpose GPUs (GPGPUs) was the year 2003, when a pair of research groups independently discovered GPU-based approaches for solving general linear algebra problems on working GPUs faster than on CPUs.</p>
</div>
</div>
</div>
<div class="sect1 text-justify">
<h2 id="_gpgpu_evolution"><a class="anchor" href="#_gpgpu_evolution"></a>2. GPGPU Evolution</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Early efforts to use GPUs as general-purpose processors required reframing computational problems in terms of graphics primitives, which were supported by two major APIs for graphics processors: OpenGL and DirectX. These were soon followed by NVIDIA&#8217;s CUDA, which allowed programmers to abandon underlying graphics concepts for more common high-performance computing concepts, such as OpenCL and other high-end frameworks. This meant that modern GPGPU pipelines could take advantage of the speed of a GPU without requiring a complete and explicit conversion of the data to
a graphical form.</p>
</div>
<div class="paragraph text-justify">
<p>NVIDIA describes CUDA as a parallel computing platform and application programming interface (API) that allows software to use specific GPUs for general-purpose processing. CUDA is a software layer that provides direct access to the GPU&#8217;s virtual instruction set and parallel computing elements for running compute cores.</p>
</div>
<div class="paragraph">
<p>Not to be outdone, AMD launched its own general-purpose computing platform in 2016, dubbed the Radeon Open Compute Ecosystem (ROCm). ROCm is primarily intended for discrete professional GPUs, such as AMD&#8217;s Radeon Pro line. However, official support is more extensive and extends to consumer products, including gaming GPUs.</p>
</div>
<div class="paragraph">
<p>Unlike CUDA, the ROCm software stack can take advantage of multiple areas, such as general-purpose GPGPU, high-performance computing (HPC), and heterogeneous computing. It also offers several programming models, such as HIP (GPU kernel-based programming), OpenMP/Message Passing Interface (MPI), and OpenCL. These also support microarchitectures, including RDNA and CDNA, for a myriad of applications ranging from AI and edge computing to IoT/IIoT.</p>
</div>
<div class="paragraph">
<p><strong>NVIDIA&#8217;s CUDA</strong></p>
</div>
<div class="paragraph text-justify">
<p>Most of NVIDIA&#8217;s Tesla and RTX series cards come with a series of CUDA cores designed to perform multiple calculations at the same time. These cores are similar to CPU cores, but they are integrated into the GPU and can process data in parallel. There can be thousands of these cores embedded in the GPU, making for incredibly efficient parallel systems capable of offloading CPU-centric tasks directly to the GPU.</p>
</div>
<div class="paragraph text-justify">
<p>Parallel computing is described as the process of breaking down larger problems into smaller, independent parts that can be executed simultaneously by multiple processors communicating through shared memory. These are then combined at the end as part of an overall algorithm. The primary purpose of parallel computing is to increase available computing power to speed up application processing and problem solving.</p>
</div>
<div class="paragraph text-justify">
<p>To this end, the CUDA architecture is designed to work with programming languages such as C, C++ and Fortran, allowing parallel programmers to more easily utilize GPU resources. This contrasts with previous APIs such as Direct3D and OpenGL, which required advanced graphics programming skills. CUDA-powered GPUs also support programming frameworks such as OpenMP, OpenACC, OpenCL, and HIP by compiling this code on CUDA.</p>
</div>
<div class="paragraph text-justify">
<p>As with most APIs, software development kits (SDKs), and software stacks, NVIDIA provides libraries, compiler directives, and extensions for the popular programming languages mentioned earlier, making programming easier and more effective. These include cuSPARCE, NVRTC runtime compilation, GameWorks Physx, MIG multi-instance GPU support, cuBLAS and many more.</p>
</div>
<div class="paragraph text-justify">
<p>A good portion of these software stacks are designed to handle AI-based applications, including machine learning and deep learning, computer vision, conversational AI, and recommender systems.</p>
</div>
<div class="paragraph text-justify">
<p>Computer vision applications use deep learning to acquire knowledge from digital images and videos. Conversational AI applications help computers understand and communicate through natural language. Recommender systems use a user&#8217;s images, language, and interests to deliver meaningful and
relevant search results and services.</p>
</div>
<div class="paragraph text-justify">
<p>GPU-accelerated deep learning frameworks provide a level of flexibility to design and train custom neural networks and provide interfaces for commonly used programming languages. All major deep learning frameworks, such as TensorFlow, PyTorch, and others, are already GPU-accelerated, so data scientists and researchers can upgrade without GPU programming.</p>
</div>
<div class="paragraph text-justify">
<p>Current use of the CUDA architecture that goes beyond AI includes bioinformatics, distributed computing, simulations, molecular dynamics, medical analytics (CTI, MRI and other scanning imaging applications ), encryption, etc.</p>
</div>
<div class="paragraph">
<p><strong>AMD&#8217;s ROCm Software Stack</strong></p>
</div>
<div class="paragraph text-justify">
<p>AMD&#8217;s ROCm software stack is similar to the CUDA platform, except it&#8217;s open source and uses the company&#8217;s GPUs to speed up computational tasks. The latest Radeon Pro W6000 and RX6000 series cards are equipped with compute cores, ray accelerators (ray tracing) and stream processors that take advantage of RDNA architecture for parallel processing, including GPGPU, HPC, HIP (CUDA-like programming model), MPI and OpenCL.</p>
</div>
<div class="paragraph text-justify">
<p>Since the ROCm ecosystem is composed of open technologies, including frameworks (TensorFlow/PyTorch), libraries (MIOpen/Blas/RCCL), programming models (HIP), interconnects (OCD), and support upstream Linux kernel load, the platform is regularly optimized. for performance and efficiency across a wide range of programming languages.</p>
</div>
<div class="paragraph text-justify">
<p>AMD&#8217;s ROCm is designed to scale, meaning it supports multi-GPU computing in and out of server-node communication via Remote Direct Memory Access (RDMA), which offers the ability to directly access host memory without CPU intervention. Thus, the more RAM the system has, the greater the processing loads that can be handled by ROCm.</p>
</div>
<div class="paragraph text-justify">
<p>ROCm also simplifies the stack when the driver directly integrates support for RDMA peer synchronization, making application development easier. Additionally, it includes ROCr System Runtime, which is language independent and leverages the HAS (Heterogeneous System Architecture) Runtime API, providing a foundation for running programming languages such as HIP and OpenMP.</p>
</div>
<div class="paragraph text-justify">
<p>As with CUDA, ROCm is an ideal solution for AI applications, as some deep learning frameworks already support a ROCm backend (e.g. TensorFlow, PyTorch, MXNet, ONNX, CuPy, etc.). According to AMD, any CPU/GPU vendor can take advantage of ROCm, as it is not a proprietary technology. This means that code written in CUDA or another platform can be ported to vendor-neutral HIP format, and from there users can compile code for the ROCm platform.</p>
</div>
<div class="paragraph text-justify">
<p>The company offers a series of libraries, add-ons and extensions to deepen the functionality of ROCm, including a solution (HCC) for the C++ programming language that allows users to integrate CPU and GPU in a single file.</p>
</div>
<div class="paragraph text-justify">
<p>The feature set for ROCm is extensive and incorporates multi-GPU support for coarse-grained virtual memory, the ability to handle concurrency and preemption, HSA and atomic signals, DMA and queues in user mode. It also offers standardized loader and code object formats, dynamic and offline compilation support, P2P multi-GPU operation with RDMA support, event tracking and collection API, as well as APIs and system management tools. On top of that, there is a growing third-party ecosystem that bundles custom ROCm distributions for a given application across a host of Linux flavors.</p>
</div>
<div class="paragraph text-justify">
<p>To further enhance the capability of exascale systems, AMD also announced the availability of its open source platform, AMD ROCm, which enables researchers to harness the power of AMD Instinct accelerators and drive scientific discovery. Built on the foundation of portability, the ROCm platform is capable of supporting environments from multiple vendors and accelerator architectures.</p>
</div>
<div class="paragraph text-justify">
<p>And with ROCm5.0, AMD extends its open platform powering the best HPC and AI applications with AMD Instinct MI200 series accelerators,
increasing ROCm accessibility for developers and delivering industry-leading performance on workloads keys. And with AMD Infinity Hub, researchers, data scientists, and end users can easily find, download, and install containerized HPC applications and ML frameworks optimized and supported on AMD Instinct and ROCm.</p>
</div>
<div class="paragraph text-justify">
<p>The hub currently offers a range of containers supporting Radeon Instinct™ MI50, AMD Instinct™ MI100, or AMD Instinct MI200 accelerators, including several applications such as Chroma, CP2k, LAMMPS, NAMD, OpenMM, etc., as well as frameworks Popular TensorFlow and PyTorch MLs. New containers are continually being added to the hub.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_amd_fusion_system_architecture"><a class="anchor" href="#_amd_fusion_system_architecture"></a>3. AMD Fusion System Architecture</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Moves to Unify CPUs and GPUs</p>
</div>
<div class="paragraph">
<p><span class="image"><a class="image" href="#fragment6"><img src="../_images/image6.png" alt="image6" width="511" height="287"></a></span></p>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer" style="border-top: 2px solid #e9e9e9; background-color: #fafafa; padding-bottom: 2em; padding-top: 2em;">
    <div class="container" style="display: flex; flex-direction: column; align-items: center; gap: 0.5em;">
        <div>
            <a href="https://www.cemosis.fr">
                <img src="../../_/img/cemosis-logo.svg" alt="Cemosis logo" height="50">
            </a>
        </div>
        <span style="font-size: 0.8rem; color: #9e9e9e">© 2025 <a href="https://www.cemosis.fr" style="text-decoration: underline;">Cemosis</a>, Université de Strasbourg</span>
    </div>
</footer>
<script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>


<script async src="../../_/js/vendor/fontawesome-icon-defs.js"></script>
<script async src="../../_/js/vendor/fontawesome.js"></script>
<script async src="../../_/js/vendor/highlight.js"></script>


<script type="text/javascript">
function toggleFullScreen() {
   var doc = window.document;
   var docEl = doc.documentElement;

   var requestFullScreen = docEl.requestFullscreen || docEl.mozRequestFullScreen || docEl.webkitRequestFullScreen || docEl.msRequestFullscreen;
   var cancelFullScreen = doc.exitFullscreen || doc.mozCancelFullScreen || doc.webkitExitFullscreen || doc.msExitFullscreen;

   if(!doc.fullscreenElement && !doc.mozFullScreenElement && !doc.webkitFullscreenElement && !doc.msFullscreenElement) {
       requestFullScreen.call(docEl);
   }
   else {
       cancelFullScreen.call(doc);
   }
}
</script>
  </body>
</html>
